{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\".\n",
    "\n",
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All). On JupyterLab, you may want to hit the \"Validate\" button as well.\n",
    "\n",
    "Caution: do not mess with the notebook's metadata; do not change a pre-existing cell's type; do not copy pre-existing cells (add new ones with the + button instead). This will break autograding; you will get a 0; you are warned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width: 100%; border: none;\" cellspacing=\"0\" cellpadding=\"0\" border=\"0\">\n",
    "  <tr>\n",
    "    <td><img src=\"https://www.planetegrandesecoles.com/wp-content/uploads/2021/07/Identite%CC%81-visuelle-Plane%CC%80te-BAC-8-600x398.png\" style=\"float: left; width: 100%\" />\n",
    "</td>\n",
    "    <td><a style=\"font-size: 3em; text-align: center; vertical-align: middle;\" href=\"https://moodle.polytechnique.fr/course/view.php?id=19260\">[CSC2S004EP - 2024] - Introduction to Machine Learning</a>\n",
    "</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bede563909330916e855da5e841daa10",
     "grade": false,
     "grade_id": "cell-b39ba8df0326a24a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": [
     "meta",
     "toc_en"
    ],
    "toc-hr-collapsed": false
   },
   "source": [
    "<a style=\"font-size: 3em;\">Lab Session 4: Overfitting and Regularization</a>\n",
    "\n",
    "Jérémie DECOCK - Adrien EHRHARDT - Jesse READ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "42b43a98048a9963d1db48a066e3b167",
     "grade": false,
     "grade_id": "cell-d0875250000514a2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "429d6da0d8592713c769985dff69140d",
     "grade": false,
     "grade_id": "cell-4660f8a8543f4de6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Earlier, we have used a **parametric model** to solve **regression problems** (lab 02: linear and polynomial regression, closed form and gradient descent, by hand and with `sklearn`!). And we used logistic regression, a **parametric model** on **classification tasks** and k-NN, a **non-parametric model** on classification **and** regression problems (in lab 03).\n",
    "\n",
    "Today we will continue the exploration of these methods, with a specific emphasis on understanding and correcting issues with overfitting, with regularization, and the bias-variance tradeoff. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f34dd89ce1d1102ec7a43a422bf26b07",
     "grade": false,
     "grade_id": "cell-c34a085df16ca768",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Note**: as in the previous labs, there are some differences in notation with the lecture slides. For instance, parameters are noted $w$ (machine learning community) in lectures but they are noted $\\theta$ here (statistics community)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b8d14dbf65fae58361ba12e77aff4de6",
     "grade": false,
     "grade_id": "cell-d67210d0da4c39f2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6aa1024fecbcee43697193b0381ba62b",
     "grade": false,
     "grade_id": "cell-9584dc80a176bfc6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn\n",
    "import sklearn.linear_model\n",
    "import sklearn.pipeline\n",
    "import sklearn.preprocessing\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "930a7ad810c9e887bc769ba61cc4b0af",
     "grade": false,
     "grade_id": "cell-e99ca5f6944a276c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Synthetic Data\n",
    "\n",
    "Toy data is not just for play. If we define and control the ground truth, we can check (and, thus, further develop) many theoretical properties of methods, in a way which is not possible with real-world data.\n",
    "\n",
    "In the following cell, provide the function `generate` which should return two 1d arrays (`x` and `y`) each of length `n` such that `x[i]` is a number generated uniformly between 0 and 10, and `y[i]` is:\n",
    "$$\n",
    "    y_i \\sim \\mathcal{N}(\\boldsymbol{\\theta}_\\star^T\\mathbf{x}_i, \\sigma^2)\n",
    "$$\n",
    "for $\\boldsymbol\\theta_\\star = [-0.6, 2]$ and $\\sigma=0.7$; where $\\mathbf{x}_i = [1, x_i]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "74b3125d7a986786b44f832349a5d485",
     "grade": false,
     "grade_id": "cell-a6891f35b7e8c3cc",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "def generate(n=20, x_min=0, x_max=10):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e24f8d7c2d01bf5f1427be0ba631891d",
     "grade": true,
     "grade_id": "cell-798856246de7bdc9",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "x, y = generate()\n",
    "assert x.shape == (20,)\n",
    "assert y.shape == (20,)\n",
    "assert x.max() <= 10\n",
    "assert x.min() >= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cc01f69a9c2922d7f48f2c6868bff700",
     "grade": true,
     "grade_id": "cell-a78008d4ebf06972",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "edaff26e7853e8f5e5bf5ea1645c03a1",
     "grade": false,
     "grade_id": "cell-9bd61843a0d8b1a1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Warmup: Ordinary Least Squares\n",
    "\n",
    "Implement ordinary least squares in the following function. It should return the vector of coefficients $\\mathbf{\\theta}$. You may ignore the `alpha` parameter (corresponding to $\\lambda$ in the lectures -- since the `lambda` keyword is already taken in Python) for now; we will use that later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "08b114fdef7792ca851ada77c5039635",
     "grade": false,
     "grade_id": "cell-e4043bb824a70658",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def least_squares(X: np.array, y: np.array, alpha: float = 0.0) -> np.array:\n",
    "    \"\"\"\n",
    "    Perform linear regression via least squares, return coefficient\n",
    "\n",
    "    :param numpy.array X: design matrix (n-sample as rows, p features as columns, may contain column of 1 to get an intercept term)\n",
    "    :param numpy.array y: response vector (n elements)\n",
    "    :param numpy.array alpha: regularization parameter (aka lambda; between 0 and inf - see Exercise 7)\n",
    "    :return: linear regression coefficient found via ols\n",
    "    :rtype: numpy.array\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a4e4b59908e89e92f99dcea337d1d022",
     "grade": false,
     "grade_id": "cell-6a894fed4eddbcfd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We build $\\mathbf{x}_i = [1, x_i]$ as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2cd78d370f7bb94c474752e8eaa6b8e6",
     "grade": false,
     "grade_id": "cell-ffb3abcaa13f8f64",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "X = np.vstack([np.ones_like(x), x]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c752dd4da5a34a508e5793977f638e49",
     "grade": false,
     "grade_id": "cell-d958decdbf40e4b6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We compute $\\hat{\\boldsymbol{\\theta}}$ as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4fe34de4662ff7f0b5313d2c9ed0183e",
     "grade": true,
     "grade_id": "cell-5585d5cb56c7f3eb",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "theta_hat = least_squares(X, y)\n",
    "assert theta_hat.shape == (2,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5b820e456576000ea24ec229c9df9eb4",
     "grade": true,
     "grade_id": "cell-8e12b42b71d72e9e",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "375bd0aeb437780c63d0e3c68329b546",
     "grade": false,
     "grade_id": "cell-740dc0b770534664",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In the following block, compute predictions on the training data and calculate the residual sum of squares (RSS) error as $\\sum_{i=1}^n (\\hat y_i - y_i)^2$, and store the result in the variable `E_RSS`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6545cb4ef182fba157511810e09db07b",
     "grade": false,
     "grade_id": "cell-3315272b29467206",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "E_RSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "781162ea5057d353814311a9f9b494e2",
     "grade": true,
     "grade_id": "cell-2fc9d9a4d7029ecf",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "648c635d5cb852be6c36d6a28943878d",
     "grade": false,
     "grade_id": "cell-58fd21d4121ec0c6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "How does this compare to the error we would get if we had available the true model (the same one used to generate the data)?\n",
    "\n",
    "Repeat the RSS calculation but for the predictions obtained from the true model $f_\\star$; and this time store the result in the variable `E_RSS_val`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1baca85882c2839f9ff9f43be835b7fe",
     "grade": false,
     "grade_id": "cell-3315272b29567206",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "theta_star = np.array([-0.6, 2])  # this is theta_star used to generate the data, see 3.\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "E_RSS_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "53f3d350cc911bdcddd72da090133e99",
     "grade": true,
     "grade_id": "cell-73bdb63563135537",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "feed6be10b7a9dac94efa5bf21bf54b5",
     "grade": false,
     "grade_id": "cell-58fd21d4121ec0c5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Those number should be pretty close, most of the time (for most data sets that you could generate), since (and it's worth taking a while to check/understand this) all assumptions for ordinary least squares, regarding the data and underlying model, have been met.\n",
    "\n",
    "Let's have a look at what is going on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bf83dd8382f689c564af57be252a0d7c",
     "grade": false,
     "grade_id": "cell-0ffaa8b3caf2421f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "n = len(x)\n",
    "X = np.vstack([np.ones_like(x),x]).T\n",
    "theta_hat = least_squares(X, y)\n",
    "yp = X @ theta_hat\n",
    "yt = X @ theta_star\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "ax.scatter(x, y)\n",
    "ax.plot(x, yp, label=\"$\\\\hat f$\")\n",
    "ax.plot(x, yt, label=\"$f_\\\\star$\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "45e6c78c0340b8508e5bc79bc046c8a0",
     "grade": false,
     "grade_id": "cell-ccf9b061c2ab589f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Unbiased Estimators \n",
    "\n",
    "In the following, check empirically if your implementation of least squares is an *unbiased estimator*.\n",
    "\n",
    "*Hint:* the derivation for this involves an expectation ($\\mathbb{E}$) operator, but empirically you can replace this with a sum over many samples. You might check the slides/notes if you need a reminder/more information about what makes a model biased or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a1f3b8550bfb3f9b77c53794673437fa",
     "grade": false,
     "grade_id": "cell-524c4c36033d5e56",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print(theta_star)\n",
    "\n",
    "# Of course, there are many ways to do this, it is the result (avg_theta) that counts... \n",
    "theta_hats = []\n",
    "for _ in range(1000):\n",
    "    # Generate dataset, estimate theta_hat, average theta_hats\n",
    "    # theta_ = [...]\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    theta_hats.append(theta_hat_)\n",
    "avg_theta = np.mean(theta_hats, axis=0)  # acts as expectation over dataset D\n",
    "avg_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c102bf9b233d01620b973eb85f768ed9",
     "grade": true,
     "grade_id": "cell-349b1c8e86894697",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "73b8ef9d9a7c62aca5a9b048f7fcfe9c",
     "grade": false,
     "grade_id": "cell-deb357fb9d8c3c2f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "So far so good. However, things are not always so easy. Assumptions are often violated. In the following we look at 'pathological cases' where least squares is not going to sufficiently solve our learning problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "22cd7d96ccbfb0659436149bef90d226",
     "grade": false,
     "grade_id": "cell-ab3ce7cb987cf9a1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "toc-hr-collapsed": false
   },
   "source": [
    "# Pathological Cases (Afflicting OLS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d269b56c38ddf04b616270d35ead1e2a",
     "grade": false,
     "grade_id": "cell-8adff8da14d09559",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Observe the effect with the following datasets. Identify the issue with each one (why does least squares fail), among:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "da2089ec6991357547d82b25c2cd9045",
     "grade": false,
     "grade_id": "cell-94d7b9c0c3b4689c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "possible_answers = {\n",
    "    \"A\": \"More features than samples\",\n",
    "    \"B\": \"y cannot be approximated by a linear combination of x\",\n",
    "    \"C\": \"Collinearity\",\n",
    "    \"D\": \"There should be a column of 1s\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8de92d91e120107a7ade06c763d480b2",
     "grade": false,
     "grade_id": "cell-5a00ad6861282ee9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "X = np.array([[1, 2, 3, 4, 5], [2, 4, 6, 8, 10]]).T\n",
    "y = np.array([1.8, 2.7, 3.4, 3.8, 3.9])\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# theta = least_squares(X, y)   # <- **TODO: UNCOMMENT**\n",
    "# theta                         # <- **TODO: UNCOMMENT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c9707f34fb64c6287ea8bc392b6eeee3",
     "grade": false,
     "grade_id": "cell-d7f00db12b5cc5a8",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# issue = \"A\" or \"B\" or ...  # <- **TODO: UNCOMMENT**\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "594e65eb79c5340339f2709a27c4f231",
     "grade": true,
     "grade_id": "cell-25146a5104ebdce1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ff0869df55419f25d1695b9074e8d156",
     "grade": false,
     "grade_id": "cell-3bcba46473e7f404",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "And what about this dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "019ee3491e58b2a80d47f6719153deac",
     "grade": false,
     "grade_id": "cell-e81d7c8d2d165e51",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "X = np.array([[1, 3, 0, 2],\n",
    "              [2, 3, 4, 4]])\n",
    "y = np.array([1.8, 2.7])\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# theta = least_squares(X, y)   # <- **TODO: UNCOMMENT**\n",
    "# theta                         # <- **TODO: UNCOMMENT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0b55098c15f0bb75d8d9048f5f40df6c",
     "grade": false,
     "grade_id": "cell-769d24f9d9d63962",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# issue = \"A\" or \"B\" or ...  # <- **TODO: UNCOMMENT**\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "350b008cc6494d578bcdd9008a1fc9c6",
     "grade": true,
     "grade_id": "cell-3b37114798de5f9a",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bf76c6e1681ad67e52572928f8fb39bf",
     "grade": false,
     "grade_id": "cell-b477da0db92e150c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "toc-hr-collapsed": false
   },
   "source": [
    "# Regularization with Ridge regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "83ba94a75b51bbdceeb6200963ccabbe",
     "grade": false,
     "grade_id": "cell-7f28009ea2cffb80",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let's try a different dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cfa12e6bbe64a10105dd89b92046bcb1",
     "grade": false,
     "grade_id": "cell-683810b3cf23cb63",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Generate some data\n",
    "np.random.seed(123)\n",
    "x = np.random.rand(10) + 0.5\n",
    "y = 1.5 * x**2 + np.random.randn(len(x)) * 0.07\n",
    "\n",
    "# Build OLS\n",
    "X = np.vstack([np.ones_like(x), x]).T\n",
    "theta_hat = least_squares(X, y)\n",
    "yp = X @ theta_hat\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "ax.scatter(x, y)\n",
    "ax.plot(x, yp)\n",
    "plt.ylim([min(y - 0.1), max(y + 0.1)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ebd71b587cf35ac1d8423fb52352d906",
     "grade": false,
     "grade_id": "cell-daa454d9a6394dd4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Take a close look at the code for generating this data. Note how the dataset is generated from a *non-linear* function.\n",
    "\n",
    "We apply basis expansion to fit a polynomial model to the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0d39949a475fc6882069e5a525a02fc6",
     "grade": false,
     "grade_id": "cell-8cb63e9ea8f4c17d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "Z = PolynomialFeatures(degree=8, include_bias=True).fit_transform(x.reshape(-1, 1))\n",
    "\n",
    "# Instantiate and fit the model\n",
    "poly_reg = sklearn.linear_model.LinearRegression(fit_intercept=False)  # bias is already in the polynomial expansion\n",
    "poly_reg.fit(Z, y)\n",
    "theta_hat_poly = poly_reg.coef_\n",
    "print(\"Coefs:\", theta_hat_poly)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "ax.scatter(x, y)\n",
    "xxx = np.linspace(min(x) - 0.1, max(x) + 0.1, 100)\n",
    "XXX = np.vstack([np.ones_like(xxx), xxx]).T\n",
    "ax.plot(xxx, XXX @ theta_hat, label=\"$\\\\hat f$\")\n",
    "zzz = PolynomialFeatures(degree=8, include_bias=True).fit_transform(xxx.reshape(-1, 1))\n",
    "plt.plot(xxx,poly_reg.predict(zzz),label=\"$\\\\hat f'$\")\n",
    "plt.ylim([min(y - 0.1), max(y + 0.1)])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "27f9f341b9d2e970759dd4d5c93ff78d",
     "grade": false,
     "grade_id": "cell-f3992f715734fd38",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Consider some of the issues here**, such as the number and magnitude of the coefficients, potential performance on the test data, in certain ranges of the $x$-domain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "893231eceb83a936d4aa2001e28f18a7",
     "grade": false,
     "grade_id": "cell-ec20d899578239b9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "A solution is to reduce the complexity of the model using a lower polynomial degree, e.g., with *ridge regularization* (a.k.a. *L2 regularization*) which applies a penalty on the values of $\\theta_j$ to constrain them to be smaller. A parameter $\\boldsymbol{\\theta}$ with small magnitude usually makes the model simpler and brings better generalization performances. This L2 regularization is included in the least square method as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e4594b9209847c79d47843197899baac",
     "grade": false,
     "grade_id": "cell-76c4f837b465db4c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "$$\n",
    "\\boldsymbol{\\theta}^*\n",
    "\\leftarrow \\arg\\min_{\\boldsymbol{\\theta}} E(\\boldsymbol{\\theta})\n",
    "\\quad \\text{with} \\quad\n",
    "E(\\boldsymbol{\\theta})\n",
    "= \\underbrace{||\\boldsymbol{y} - \\boldsymbol{X} \\boldsymbol{\\theta}||^2_2}_{\\text{error term}} ~~ + \\underbrace{\\lambda ||\\boldsymbol{\\theta}||^2_2}_{\\text{regularization}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "20862f2c91fe90e59b66793936c2021d",
     "grade": false,
     "grade_id": "cell-c6c6debf5100b610",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "where $\\lambda \\in \\mathbb{R}^+$ is the *regularization strength* coefficient:\n",
    "- when $\\lambda$ goes to infinity, the regularization term dominates the error term (MSE) and the coefficients $\\boldsymbol{\\theta}$ tend to zero;\n",
    "- when $\\lambda$ goes to 0, the regularization term looses the importance and eventually the regularization term is ignored;\n",
    "- $\\lambda$ is a *meta* or *hyper parameter*;\n",
    "- the best $\\lambda$ for a problem can be estimated empirically, we look at this soon!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b142b1b4eca29ec0b2bc74e98a226308",
     "grade": false,
     "grade_id": "cell-af103435b049e13f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "On a sheet of paper or in the cell below (either with LaTeX or by embedding an image):\n",
    "- Compute the analytic formulation of the gradient $\\nabla_{\\boldsymbol{\\theta}} E(\\boldsymbol{\\theta})$\n",
    "- Compute the analytic formulation of the optimal parameter $\\boldsymbol{\\theta^*}$\n",
    "\n",
    "*Note*: this is very similar to [`lab_session_02`](https://adimajo.github.io/CSE204-2022-admin/lab_session_02.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d80b9338facf6d2ee983077b5ae5acd8",
     "grade": false,
     "grade_id": "cell-b9177a2ef8ba3b25",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Recall:\n",
    "\n",
    "$$\n",
    "(\\boldsymbol{A} + \\boldsymbol{B})^T = \\boldsymbol{A}^T + \\boldsymbol{B}^T\n",
    "$$\n",
    "\n",
    "$$\n",
    "(\\boldsymbol{A} \\boldsymbol{B})^T = \\boldsymbol{B}^T \\boldsymbol{A}^T\n",
    "$$\n",
    "\n",
    "\\begin{align}\n",
    "\\text{Associativity:} & \\quad\n",
    "(\\boldsymbol{A} \\boldsymbol{B}) \\boldsymbol{C}\n",
    "= \\boldsymbol{A} (\\boldsymbol{B} \\boldsymbol{C})\n",
    "= \\boldsymbol{A} \\boldsymbol{B} \\boldsymbol{C}\n",
    "\\\\\n",
    "\\text{Non-commutativity:} & \\quad\n",
    "\\boldsymbol{A} \\boldsymbol{B}\n",
    "\\neq \\boldsymbol{B} \\boldsymbol{A}\n",
    "\\\\\n",
    "\\text{Distributivity:} & \\quad\n",
    "\\boldsymbol{A} (\\boldsymbol{B} + \\boldsymbol{C})\n",
    "= \\boldsymbol{A} \\boldsymbol{B} + \\boldsymbol{A} \\boldsymbol{C} \\\\\n",
    "& \\quad\n",
    "(\\boldsymbol{B} + \\boldsymbol{C}) \\boldsymbol{A} \n",
    "= \\boldsymbol{B} \\boldsymbol{A} + \\boldsymbol{C} \\boldsymbol{A}\n",
    "\\end{align}\n",
    "\n",
    "Differential identities for matrices:\n",
    "\n",
    "\\begin{align}\n",
    "d(\\boldsymbol{A}) & = 0  \\quad \\quad \\quad \\text{if } \\boldsymbol{A} \\text{ is not function of } \\boldsymbol{X} \\\\\n",
    "d(a\\boldsymbol{X}) & = a d \\boldsymbol{X}  \\quad \\quad \\text{if } a \\text{ is not function of } \\boldsymbol{X} \\\\\n",
    "d(\\boldsymbol{X} +\\boldsymbol{Y}) & = d \\boldsymbol{X} + d \\boldsymbol{Y} \\\\\n",
    "d(\\boldsymbol{XY}) & = (d \\boldsymbol{X}) \\boldsymbol{Y} + \\boldsymbol{X} (d \\boldsymbol{Y}) \\\\\n",
    "d(\\boldsymbol{X}^T) & = (d \\boldsymbol{X})^T \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b9bdcaff856319076fbb812998f16c1a",
     "grade": true,
     "grade_id": "cell-d0e38e92f00050b9",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "867aaf8f448c967e6e50aa753411f453",
     "grade": false,
     "grade_id": "cell-ce81f172c59bfed2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Question**: Is it a convex optimization problem like *Ordinary Least Squares*? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cd5124c8bf6e07568acbcaf230a6571c",
     "grade": true,
     "grade_id": "cell-e55214b8bf672815",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f367d4692bd8efe98fd588988d8efdec",
     "grade": false,
     "grade_id": "cell-6e62cf87e0f1a320",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Check the following Scikit Learn implementation of the Ridge Regression (more info here: https://scikit-learn.org/stable/modules/linear_model.html#ridge-regression ). In particular: Change the `alpha` ($\\lambda$) and `k`/degree parameters, and observe the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyper-parameters\n",
    "alpha = 0  # regularization #TODO change to, e.g., 0.1, 1, etc.\n",
    "k = 8  # degree polynomial  #TODO change to, e.g., 8, 4, 2, etc.\n",
    "Z = PolynomialFeatures(degree=k, include_bias=True).fit_transform(x.reshape(-1, 1))\n",
    "\n",
    "# Fit ridge regression\n",
    "poly_ridge = sklearn.linear_model.Ridge(alpha=alpha, fit_intercept=False)\n",
    "poly_ridge.fit(Z, y)\n",
    "\n",
    "# Have a look at the coefficients\n",
    "coefs = [poly_ridge.intercept_] + poly_ridge.coef_\n",
    "print(\"Coefs:\", coefs)\n",
    "\n",
    "# Plot the result\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "ax.scatter(x, y)\n",
    "xxx = np.linspace(min(x) - 0.1, max(x) + 0.1, 100)\n",
    "zzz = PolynomialFeatures(degree=k, include_bias=True).fit_transform(xxx.reshape(-1, 1))\n",
    "plt.plot(xxx,poly_ridge.predict(zzz),label=\"$\\\\hat f$ (deg. %d, $\\\\lambda=%3.2f$)\" % (k, alpha), color='orange')\n",
    "plt.ylim([min(y - 0.1), max(y + 0.1)])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cc0c656b536bb12c7daf1f4495de27c8",
     "grade": false,
     "grade_id": "cell-e3cd91daee05a3e6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In the following, plot the ridge coefficients as a function of the regularization parameter.\n",
    "\n",
    "Evaluate the following sequence of regularization strength: `alphas = np.logspace(-2, 5, 50)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "724b2f7533b155f565fad83fa1c18f60",
     "grade": false,
     "grade_id": "cell-192bb9689974cff6",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Compute paths\n",
    "alphas = np.logspace(-2, 5, 50)\n",
    "\n",
    "coefs = []\n",
    "for a in alphas:\n",
    "    # Fit a `Ridge` object\n",
    "    # coefs.append(...)  # TO UNCOMMENT: append the ridge coefficients to the `coefs` list\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e025f9d062a45d73b67c9a4359dc39e1",
     "grade": true,
     "grade_id": "cell-db41f1411f0260ee",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Display results\n",
    "fig, ax = plt.subplots(figsize=(18, 8))\n",
    "ax.plot(alphas, coefs)\n",
    "ax.set_xscale('log')\n",
    "plt.xlabel('Regularization')\n",
    "plt.ylabel('Coefficients')\n",
    "plt.title('Ridge coefficients as a function of the regularization')\n",
    "plt.axis('tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9e5eff34c2e36ce3cd82e49b087da439",
     "grade": false,
     "grade_id": "cell-acdb44b8be0f5f90",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Return to the function `least_squares` that you implemented already above**; now update the function to implement the ridge regression in Python (*without* using Scikit-Learn). The ridge/regularization should 'take effect' *only when* `alpha > 0`. Observe your result in the following (you can experiment with `k` and `alpha` again):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the hyper-parameters\n",
    "k = 2\n",
    "alpha = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "99b4ec2d932c15f6a374c69eaa2e991b",
     "grade": true,
     "grade_id": "cell-6c21d1c6b41ee2b5",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test your function\n",
    "Z = PolynomialFeatures(degree=k, include_bias=True).fit_transform(x.reshape(-1, 1))\n",
    "theta_ridge = least_squares(Z, y, alpha=alpha)\n",
    "print(\"Coefs:\", theta_ridge)\n",
    "\n",
    "# Plot the results\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "ax.scatter(x, y)\n",
    "xxx = np.linspace(min(x) - 0.1, max(x) + 0.1, 100)\n",
    "zzz = PolynomialFeatures(degree=k, include_bias=True).fit_transform(xxx.reshape(-1, 1))\n",
    "plt.plot(xxx,zzz @ theta_ridge, label=\"$\\\\hat f$ (deg. %d, $\\\\lambda=%3.2f$)\" % (k,alpha), color='salmon')\n",
    "plt.ylim([min(y - 0.1), max(y + 0.1)])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b3c50146c46b0de718168a8703325053",
     "grade": false,
     "grade_id": "cell-294e695f17e8ebc3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Ridge Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d25eda999641f7998aa28a8c0f400674",
     "grade": false,
     "grade_id": "cell-4b7455d8a6881c65",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In the following we generate and plot a toy dataset for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "461aaed69570ac13f2cd738cb486fcab",
     "grade": false,
     "grade_id": "cell-047253420253af9e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def gen_2d_classification_samples(n: int = 20) -> np.array:\n",
    "    \"\"\"\n",
    "    Generates 2-dimensional samples, which belong to either of 2 classes\n",
    "\n",
    "    :param int n: number of samples to draw per class\n",
    "    :returns: two arrays of X (shape (n,2)) and y (shape (n,)\n",
    "    \"\"\"\n",
    "    y = ((np.random.rand(n) >= 0.5) * 1).astype(int)\n",
    "    X = np.random.randn(n, 2) * 0.5 + (y.reshape(n, -1) == 1) * 1\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "66137d284bc523bb90bd8181ae3c556d",
     "grade": false,
     "grade_id": "cell-527c211896a03562",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "X,y = gen_2d_classification_samples(100)\n",
    "x_min, x_max = np.array((X[:, 0], X[:, 1]))[0, :].min() - .5, np.array((X[:, 0], X[:, 1]))[0, :].max() + .5\n",
    "y_min, y_max = np.array((X[:, 0], X[:, 1]))[1, :].min() - .5, np.array((X[:, 0], X[:, 1]))[1, :].max() + .5\n",
    "h = .02  # step size in the mesh\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "plt.xlabel('$x_1$')\n",
    "plt.ylabel('$x_2$')\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "40b0815218943960bd777625e689d069",
     "grade": false,
     "grade_id": "cell-c1f8e06719bc7c7d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We used gradient descent to fit a Logistic Regression, which finds a linear decision boundary between these two classes, and obtained the same results as `sklearn` in lab 03."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d7542c945c8c575e2aaaa2fa12cf3c1e",
     "grade": false,
     "grade_id": "cell-f55bc257ae956696",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Recall the `LogisticRegression` class:\n",
    "logistic_regression = sklearn.linear_model.LogisticRegression(C=1e9).fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "90e4cf741e9dfea915c42a3112d6a73c",
     "grade": false,
     "grade_id": "cell-b663c5f93767813b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "Z = logistic_regression.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 0]\n",
    "plt.pcolormesh(xx, yy, (Z.reshape(xx.shape) > 0.5) * 1, cmap=plt.cm.Paired)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8cf37e40bfcbb5531838edc8d06e5857",
     "grade": false,
     "grade_id": "cell-41bfb2bd4c75d08b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let's try to do polynomial logistic regression, with the same methodology: (1) fit a `PolynomialFeatures` object, that is, a polynomial expansion of x, from 1 = `x**0` to `x**degree`, (2) use that as an input to `LogisticRegression` (without an intercept term)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "04a4df7f3754941497c91cf5c0d4c1e8",
     "grade": false,
     "grade_id": "cell-0f9df8f4245344cf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "fil = PolynomialFeatures(degree=7, include_bias=True)\n",
    "Phi = fil.fit_transform(X)\n",
    "\n",
    "logistic_regression = sklearn.linear_model.LogisticRegression(\n",
    "    C=1e9, solver='liblinear', fit_intercept=False).fit(Phi, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4c23d5a9a4f014182ffd958e50d55318",
     "grade": false,
     "grade_id": "cell-ac7688a6627de180",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "xxxx = np.c_[xx.ravel(), yy.ravel()]\n",
    "phi_ = fil.transform(xxxx)\n",
    "Z = logistic_regression.predict_proba(phi_)[:, 0]\n",
    "plt.pcolormesh(xx, yy, (Z.reshape(xx.shape) > 0.5) * 1, cmap=plt.cm.Paired)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cfd978d1608d46262cac50224c8b3e7f",
     "grade": false,
     "grade_id": "cell-803e9cbca516b734",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "You might observe a relatively poor fit, in the sense that it probably won't *generalize* well. Just like linear regression, let's try to regularize logistic regression with a ridge penalty ($\\lambda ||\\boldsymbol{\\theta}||_2^2$).\n",
    "On a sheet of paper (that you drag-and-drop in the cell below or in $\\LaTeX$), compute the analytic formulation of the gradient $\\nabla_{\\boldsymbol{\\theta}} E(\\boldsymbol{\\theta})$.\n",
    "\n",
    "*Note*: this is similar to above **but** the error function is now the log-loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2607a8024de3238b83db8ec73a8509d7",
     "grade": true,
     "grade_id": "cell-052a5d32e545bd10",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e2ae3dbed0d0ff4b5c43028149b2d48b",
     "grade": false,
     "grade_id": "cell-a0259d90c5f7d054",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We can use the `C` parameter in `sklearn`, inversely proportional to $\\lambda$, to fit such a penalization. Play around with `C` in the following cell, than plot the resulting decision boundary in the subsequent cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lower C means more regularization (higher alpha) - you may edit C:\n",
    "logistic_regression = sklearn.linear_model.LogisticRegression(\n",
    "    C=0.1, solver='liblinear', penalty='l2', fit_intercept=False).fit(Phi,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ce2ed2eac631367fe249c3ad1fdb9483",
     "grade": false,
     "grade_id": "cell-d199c9f91bf4e189",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "xxxx = np.c_[xx.ravel(), yy.ravel()]\n",
    "phi_ = fil.transform(xxxx)\n",
    "Z = logistic_regression.predict_proba(phi_)[:, 0]\n",
    "plt.pcolormesh(xx, yy, (Z.reshape(xx.shape) > 0.5) * 1, cmap=plt.cm.Paired)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2e9fd79e37af9e81d8b4ed9e471b712c",
     "grade": false,
     "grade_id": "cell-4d80f2579f3057f0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Cross Validation and Hyper-Parameter Tuning\n",
    "\n",
    "What value of `C` should we use? Consider the potential values listed in `c_range` in the following code block. Make use of the `cross_val_score` function which will handle cross validation for you. You should do 10-fold cross validation, under the 'accuracy' scoring metric; use logistic regression as the `estimator`, and the `PolynomialFeatures` (degree 7) `Phi`, as produced recently (above). Consider the average accuracy across the 10 folds as the 'value' of each `C`-value; choose the one you prefer and put it in `c_selected`.\n",
    "\n",
    "*Hint*: see [cross_val_score](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "764eb764e78da36530d27ee26990bc83",
     "grade": false,
     "grade_id": "cell-07006aabb945c193",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This is the kind of model you should test on: \n",
    "# sklearn.linear_model.LogisticRegression(C=c, solver='liblinear', penalty='l2')\n",
    "\n",
    "# ... for the following values of c: \n",
    "c_range = [0.001, 0.01, 0.1, 1, 10, 50, 100, 200, 300, 400, 500, 1000]\n",
    "\n",
    "# Make use of the following \n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7739e2e619b7b838e7bf30ea40b8da69",
     "grade": false,
     "grade_id": "cell-5b7ddea43a24b9dd",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# You should update the following variable with your choice\n",
    "c_selected = c_range[0]\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a8deeed864fa5f3cdf265d1ca29df375",
     "grade": true,
     "grade_id": "cell-4fa69b5912bdeb7a",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ebcf4c4da8015feefd90d96d17351dd1",
     "grade": false,
     "grade_id": "cell-d6a6b661bbb7f0e6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "toc-hr-collapsed": false
   },
   "source": [
    "# Weighted Least Squares (Bonus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1b6f2a01d7275c00a9a696c2a7af43f8",
     "grade": false,
     "grade_id": "cell-3a63a960d5a8b724",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def gen_1d_polynomial_regression_samples(n_samples: int = 15) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generate 1-dimensional regression samples (x, y)\n",
    "\n",
    "    :param int n_samples: how many samples to return\n",
    "    \"\"\"\n",
    "    x = np.random.uniform(low=0., high=1.5, size=n_samples)\n",
    "    y = np.cos(2. * np.pi * x) + np.random.normal(scale=0.1, size=x.shape)\n",
    "    df = pd.DataFrame(np.array([x, y]).T, columns=['x', 'y'])\n",
    "    df = sklearn.utils.shuffle(df).reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "621512ca79f0a55e2e3ebd7222db07a3",
     "grade": false,
     "grade_id": "cell-58f4b97740e11f2b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def plot_regression_1d(X, y, theta=None, x_min=0, x_max=2):\n",
    "    \"\"\"\n",
    "    Plot linear regression of X on y given theta\n",
    "    \"\"\"\n",
    "    assert X.ndim == 2 and X.shape[1] == 2, X.shape\n",
    "    assert y.ndim == 2 and y.shape[1] == 1, y.shape\n",
    "    if theta is not None:\n",
    "        assert theta.ndim == 2 and theta.shape == (2, 1), theta.shape\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(X[:,1], y)\n",
    "\n",
    "    if theta is not None:\n",
    "        x = np.linspace(x_min, x_max, 50)\n",
    "        y = theta[0] + theta[1] * x\n",
    "\n",
    "        ax.plot(x, y, \"--r\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4b2aa743bd0856909a3a6212da1a3152",
     "grade": false,
     "grade_id": "cell-d8cac13da15a6d08",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def plot_1d_regression_samples(dataframe: pd.DataFrame, model=None):\n",
    "    \"\"\"\n",
    "    Plot the data in dataframe, as wellas (optionnally) the predictions from a model\n",
    "\n",
    "    :param pandas.DataFrame dataframe: dataframe containing 'x' and 'y'\n",
    "    :param model: model to predict\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    \n",
    "    df = dataframe.copy()  # make an alias\n",
    "    \n",
    "    ERROR_MSG1 = \"The `dataframe` parameter should be a Pandas DataFrame having the following columns: ['x', 'y']\"\n",
    "    assert df.columns.values.tolist() == ['x', 'y'], ERROR_MSG1\n",
    "    \n",
    "    if model is not None:\n",
    "        \n",
    "        # Compute the model's prediction\n",
    "        x_pred = np.linspace(df.x.min(), df.x.max(), 100).reshape(-1, 1)\n",
    "        y_pred = model.predict(x_pred)\n",
    "        df_pred = pd.DataFrame(np.array([x_pred.flatten(), y_pred.flatten()]).T, columns=['x', 'y'])\n",
    "        df_pred.plot(x='x', y='y', style='r--', ax=ax)\n",
    "\n",
    "    # Plot also the training points\n",
    "    df.plot.scatter(x='x', y='y', ax=ax)\n",
    "    delta_y = df.y.max() - df.y.min()\n",
    "    plt.ylim((df.y.min() - 0.15 * delta_y,\n",
    "              df.y.max() + 0.15 * delta_y))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cb831899a431c66d2b9d6815ed02272a",
     "grade": false,
     "grade_id": "cell-af4bac9082491cae",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "For some regression problems, it may be helpful to give different importance to examples in the *learning set* $\\mathcal{D} = \\{(\\boldsymbol{x^{(i)}}, y^{(i)})\\}_{1 \\leq i \\leq n}$ that is to say associate a weight $\\omega^{(i)}$ to example $\\boldsymbol{x}^{(i)}$ in order to prioritize some of them and ignore some others (e.g. outliers).\n",
    "Introducing these weights in the method of Least Square, the regression problem becomes:\n",
    "\n",
    "$$E(\\boldsymbol{\\theta}) = \\sum_{i=1}^n \\omega^{(i)} (y^{(i)} - \\boldsymbol{x}^{(i)} \\boldsymbol{\\theta})^2.$$\n",
    "\n",
    "In order to use the matrix notation, we put weights $\\omega^{(i)}$ in the diagonal of the following matrix $\\Omega$:\n",
    "\n",
    "$$\n",
    "\\Omega =\n",
    "\\begin{pmatrix}\n",
    "\\omega^{(1)} & 0            & \\cdots & 0 \\\\\n",
    "0            & \\omega^{(2)} & \\cdots & 0 \\\\\n",
    "\\vdots       & \\vdots       & \\ddots & \\vdots \\\\\n",
    "0            & 0            & \\cdots & \\omega^{(n)} \\\\\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "da6593221fc577e551b7142af031ba00",
     "grade": false,
     "grade_id": "cell-29c5ef0230f5883c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Then we can write:\n",
    "\n",
    "$$E(\\boldsymbol{\\theta}) = (\\boldsymbol{y} - \\boldsymbol{X} \\boldsymbol{\\theta})^T \\Omega (\\boldsymbol{y} - \\boldsymbol{X} \\boldsymbol{\\theta})$$\n",
    "\n",
    "with:\n",
    "$$\n",
    "\\boldsymbol{X} = \\begin{pmatrix} 1 & x_1^{(1)} & \\dots & x_p^{(1)} \\\\ \\vdots & \\vdots & \\dots & \\vdots \\\\ 1 & x_1^{(n)} & \\dots & x_p^{(n)} \\end{pmatrix}\n",
    "\\quad \\quad\n",
    "\\boldsymbol{y} = \\begin{pmatrix} y^{(1)} \\\\ \\vdots \\\\ y^{(n)} \\end{pmatrix}\n",
    "\\quad \\quad\n",
    "\\boldsymbol{\\theta} = \\begin{pmatrix} \\theta_0 \\\\ \\vdots \\\\ \\theta_p \\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d3cdff9c950b9a73a9e75d9cc3ec7031",
     "grade": false,
     "grade_id": "cell-d57c2ce27abd8717",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "\n",
    "On a sheet of paper (that you drag-and-drop in the following cell or with $\\LaTeX$):\n",
    "- Compute the analytic formulation of the gradient $\\nabla_{\\boldsymbol{\\theta}} E(\\boldsymbol{\\theta})$\n",
    "- Compute the analytic formulation of the optimal parameter $\\boldsymbol{\\theta^*}$\n",
    "\n",
    "*Note*: this is again very similar to question 2.1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2d0a087047b7ae77feb7cf901867357f",
     "grade": true,
     "grade_id": "cell-db6dea2cff4057b4",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2665d4e9b4e2aaf276d151526433d7b9",
     "grade": false,
     "grade_id": "cell-a8401a83c5f0f92e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Is this problem a convex optimization problem like *Ordinary Least Squares*? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a095a357be6c4bda61110f73483fd430",
     "grade": true,
     "grade_id": "cell-ae3362193d658468",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9f4237a8c151ee9cb6cf5269bf699dc3",
     "grade": false,
     "grade_id": "cell-5b83b793a648b904",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We have the following dataset and weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3be70e75c82854462b842272e490d357",
     "grade": false,
     "grade_id": "cell-9232b23cbb354822",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "X = np.array([[1, 1],\n",
    "              [1, 2],\n",
    "              [1, 3],\n",
    "              [1, 4],\n",
    "              [1, 5]])\n",
    "\n",
    "y = np.array([1.8, 4.5, 3.4, 3.6, 4.2]).reshape([-1, 1])\n",
    "\n",
    "Omega = np.diag([1, 2, 3, 2, 1])\n",
    "\n",
    "Omega"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9c94f23b2c9e7c130b4515b9f9fbf3ba",
     "grade": false,
     "grade_id": "cell-21a6c3f6d25fc9a6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(X[:,1], y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1b4dfa6c6474ee3186d862ef4b5cdcde",
     "grade": false,
     "grade_id": "cell-42925952786b1b54",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Complete the following Python implementation of the `weighted_least_squares()` procedure.\n",
    "It should return the optimal parameter $\\boldsymbol{\\theta^*}$ using the method of Least Square for the matrix of weights $\\Omega$ defined above. Here, we expect $\\theta$ to be a Numpy array of shape `(1, 2)` (i.e. a vector of two elements)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "468578b268296bdd06340969a303542b",
     "grade": false,
     "grade_id": "cell-9ff0f17bcf391a48",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "*Numpy hints*:\n",
    "- The transpose of a matrix `X` is obtained with `X.T`\n",
    "- The inverse of a matrix `X` is obtained with `np.linalg.inv(X)`\n",
    "- The product of two matrices `X` and `Y` is obtained with `np.dot(X, Y)` or `np.matmul(X, Y)` or `X @ Y`\n",
    "- The dot product of a matrix `X` and a vector `y` is obtained with `np.dot(X, y)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "17b66c35f561f48a7dca2d4985ecc7c7",
     "grade": false,
     "grade_id": "cell-43283b079c1fc396",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def weighted_least_squares(X, Omega, y):\n",
    "    # theta = ...  # TO UNCOMMENT AND COMPLETE\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9b0c5876d2eb2d9b7e6124261fb9664e",
     "grade": true,
     "grade_id": "cell-becb4b7cff3023f2",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "theta = weighted_least_squares(X, Omega, y)\n",
    "assert len(theta) == 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ccd8eab7390d5145c18bc5065b60d1ee",
     "grade": false,
     "grade_id": "cell-f5d7e39b2fb99f6d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Check graphically your model using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7042ec68d6a4bb9700db5e7544d2ca42",
     "grade": false,
     "grade_id": "cell-719edd179ad09de4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "plot_regression_1d(X, y, theta, x_min=0, x_max=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9e109e95d6373d2b59243fedc4a720ca",
     "grade": false,
     "grade_id": "cell-9054a849b5c9a6fd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "This is arguably relatively bad, since it seems the second point might be an outlier and \"drags\" the regression line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "04b82484c3bfe039bf686db3c16a32cd",
     "grade": false,
     "grade_id": "cell-d6d83d2ef2b2449b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Change the weights in $\\Omega$ to ignore the second point $x = 2$ (give the same weight to all other points) then recompute $\\theta$ using `weighted_least_squares()` and check the results on plots with `plot_regression_1d()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0c38c49dcefa62c1a2519ac62209b4ef",
     "grade": false,
     "grade_id": "cell-dbf64f2c60e958a8",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Omega = ...\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "theta = weighted_least_squares(X, Omega, y)\n",
    "plot_regression_1d(X, y, theta, x_min=0, x_max=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c49d7b7e042b66f6681c1c993f165810",
     "grade": true,
     "grade_id": "cell-34174ea85ab64074",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert len(theta) == 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "224035269e69b8afb9b0e4d0e9c4b2a8",
     "grade": false,
     "grade_id": "cell-fbc1e8a58e25ddb4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "toc-hr-collapsed": false
   },
   "source": [
    "# Nadaraya-Watson Kernel Regression (Bonus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "40587116f3cd42181b34892260e0466a",
     "grade": false,
     "grade_id": "cell-dbde0d367af37740",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Like k-Nearest Neighbors, *Nadaraya-Watson Kernel Regression* is a **non-parametric** model, i.e. decisions are made according to known examples from the *learning set* $\\mathcal{D} = \\{(y^{(i)}, \\boldsymbol{x^{(i)}})\\}_{1 \\leq i \\leq n}$ of $n$ examples and considering a kind of proximity relationship.\n",
    "\n",
    "With k-Nearest Neighbors, decisions are based only on the closest neighbors and other examples are simply ignored. If you remember correctly from the end of Lab 03, we used `sklearn`'s implementation of k-Nearest Neighbors and its `weights=\"distance\"` option to use the label of the nearest neighbors **proportionally to their distance from the point to predict**.\n",
    "Contrary to standard k-NN but in the same fashion as the aforementioned option, with Kernel Regression all examples $(\\boldsymbol{x}^{(i)}, y^{(i)})$ from $\\mathcal{D}$ are used to predict the label $y$ of any new point $\\boldsymbol{x}$, but their respective contribution in this prediction is weighted using a *kernel function* $K(\\boldsymbol{x}^{(i)}, \\boldsymbol{x})$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c732cfff6b998fbc9a931288cd137491",
     "grade": false,
     "grade_id": "cell-2b311266839dc8ae",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "$$\n",
    "\\hat{y}\n",
    "= f(\\boldsymbol{x})\n",
    "= \\frac{\\sum^{n}_{i=1} K(\\boldsymbol{x}^{(i)}, \\boldsymbol{x}) ~ y^{(i)}}{\\sum^{n}_{j=1} K(\\boldsymbol{x}^{(j)}, \\boldsymbol{x})}\n",
    "= \\sum^{n}_{i=1} y^{(i)} \\omega^{(i)}\n",
    "$$\n",
    "\n",
    "with $\\sum^{n}_{i=1} \\omega^{(i)} = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0ae51afbc5620ed25b839f272b2b864c",
     "grade": false,
     "grade_id": "cell-2c49c814291d1f6e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Recall about the notation used here:\n",
    "- $\\boldsymbol{x}^{(i)}$ is the feature (input) vector of the $i^{\\text{th}}$ example in $\\mathcal{D}$ (and $y^{(i)}$ is its label). Beware: $\\boldsymbol{x}^{(i)}$ is not the $i^{\\text{th}}$ power of $\\boldsymbol{x}$ (we will write $\\boldsymbol{x}^{(i)2}$ for the square of $\\boldsymbol{x}^{(i)}$)!\n",
    "- $x_i$ is the value of $\\boldsymbol{x}$ on the $i^{\\text{th}}$ dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "438832a224d7b9b4a7deb41dc4db3cda",
     "grade": false,
     "grade_id": "cell-dd6b5043a04876a3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Implement the Gaussian kernel $K$ in the following `gaussian_kernel()` Python function.\n",
    "\n",
    "$$\n",
    "K(\\boldsymbol{u}, \\boldsymbol{v})\n",
    "= \\exp\\left(\\frac{-||\\boldsymbol{u} - \\boldsymbol{v}||^2_2}{2 \\sigma^2} \\right)\n",
    "$$\n",
    "\n",
    "where $\\sigma$ is a parameter equal to $1$ by default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e275b8f9b5a9cbb004e7871acd9d3cdc",
     "grade": false,
     "grade_id": "cell-79b3be4f27f76764",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "You can assume $u$ and $v$ to be simple scalars to simplify this Python implementation (i.e. restrict yourself to regression problem with 1 dimension inputs $x \\in \\mathbb{R}$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ea5accd1306c5144ea89276732ad7c84",
     "grade": false,
     "grade_id": "cell-427a677796779ec4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Recall: $e^x$ is written `math.exp(x)` in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8cb92eedbe5d93933a3a20121cc66356",
     "grade": false,
     "grade_id": "cell-3128fc886c8b2d96",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def gaussian_kernel(u, v, sigma = 1.):\n",
    "    # return expression above\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a7f58e2dec95ee9f32c2fded6234ffaf",
     "grade": true,
     "grade_id": "cell-de16122654c5da31",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert gaussian_kernel(0,0) == 1\n",
    "assert gaussian_kernel(1,0) == math.exp(-1/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5b8c9841b71acbd577f089a8cf2148b6",
     "grade": false,
     "grade_id": "cell-0f7375430d795b1a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Implement the Nadaraya-Watson kernel regression in the following `kernel_regression()` Python function.\n",
    "\n",
    "$$\n",
    "\\text{kernel\\_regression}(\\boldsymbol{x}, \\mathcal{D})\n",
    "= \\frac{\\sum^{n}_{i=1} K(\\boldsymbol{x}^{(i)}, \\boldsymbol{x}) y^{(i)}}{\\sum^{n}_{j=1} K(\\boldsymbol{x}^{(j)}, \\boldsymbol{x})}\n",
    "= \\hat{y}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e3ecbb6237bd7054ec4af534a1b4701b",
     "grade": false,
     "grade_id": "cell-ace6c2a29b8cbb05",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "You can assume that $x$ is a scalar to simplify the Python implementation.\n",
    "We assume `dataset` contains examples $\\mathcal{D} = \\{(\\boldsymbol{x^{(i)}}, y^{(i)})\\}_{1 \\leq i \\leq n}$ in a Pandas DataFrame having:\n",
    "- one row per example\n",
    "- a column \"x\" containing the examples' features (only one dimension here)\n",
    "- a column \"y\" containing the examples' labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e27c266e02ddd24a3688fe54ea808c69",
     "grade": false,
     "grade_id": "cell-297c79ad23997bb6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Hint**: you can use the following `for` loop to compute $\\sum K(\\boldsymbol{x}^{(i)}, \\boldsymbol{x}) ~ y^{(i)}$: `for xi, yi in zip(df.x, df.y)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b629ed36c1933e7fa46a49041d71b286",
     "grade": false,
     "grade_id": "cell-e5f3b332a9759b1c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def kernel_regression(x, dataset):\n",
    "    # Hint: calculate numerator and denominator separately with list comprehensions\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5606befcc72587c09c9f22c1bd5586cc",
     "grade": true,
     "grade_id": "cell-7503a104c8598d2d",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert kernel_regression(1, pd.DataFrame({\"x\": [0], \"y\": [0]})) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4369d113f3030ff8f92167a5fd890edf",
     "grade": false,
     "grade_id": "cell-1a7ed3c83d4c49a5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We have the following dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d862b1a824722654318f3145e0aadea0",
     "grade": false,
     "grade_id": "cell-8704f92203357312",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "dataset = pd.DataFrame([[2., 0.],\n",
    "                        [5., 2.],\n",
    "                        [7., 1.],\n",
    "                        [10., 2.],\n",
    "                        [14., 4.],\n",
    "                        [16., 3.],\n",
    "                        [17., 0.]], columns=['x', 'y'])\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c63e8af716071bc2aba625013b8aef62",
     "grade": false,
     "grade_id": "cell-ede96d6dd3db8310",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Check your `kernel_regression()` function with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "58e323b10963408876652f1a89daed9b",
     "grade": false,
     "grade_id": "cell-72989626d17f0fb2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "x_pred = np.linspace(0., 20., 200)\n",
    "y_pred = [kernel_regression(x, dataset) for x in x_pred]\n",
    "\n",
    "ax = dataset.plot.scatter(x='x', y='y', label=\"Dataset\", figsize=(12,8))\n",
    "ax.plot(x_pred, y_pred, \"-r\", label=\"Kernel regression\")\n",
    "plt.legend();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
