{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\".\n",
    "\n",
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All). On JupyterLab, you may want to hit the \"Validate\" button as well.\n",
    "\n",
    "Caution: do not mess with the notebook's metadata; do not change a pre-existing cell's type; do not copy pre-existing cells (add new ones with the + button instead). This will break autograding; you will get a 0; you are warned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width: 100%; border: none;\" cellspacing=\"0\" cellpadding=\"0\" border=\"0\">\n",
    "  <tr>\n",
    "    <td><img src=\"https://www.planetegrandesecoles.com/wp-content/uploads/2021/07/Identite%CC%81-visuelle-Plane%CC%80te-BAC-8-600x398.png\" style=\"float: left; width: 100%\" />\n",
    "</td>\n",
    "    <td><a style=\"font-size: 3em; text-align: center; vertical-align: middle;\" href=\"https://moodle.polytechnique.fr/course/view.php?id=19260\">[CSC2S004EP - 2024] - Introduction to Machine Learning</a>\n",
    "</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "5YuK2f4u_CzI",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c198a3cdae4b9403198bd0e945116457",
     "grade": false,
     "grade_id": "cell-d65e9154014fb110",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a style=\"font-size: 3em;\">Lab Session 6: Building a Neural Network From Scratch</a>\n",
    "\n",
    "J.B. Scoggins - Adrien Ehrhardt - Jesse Read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "rWMgDSTC_CzL",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f40863018f50cc007a1d9183eae0b5ca",
     "grade": false,
     "grade_id": "cell-6b9fe28aa4a433c9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Introduction\n",
    "\n",
    "In this lab you will learn to create a simple multilayer perceptron (MLP) neural network from scratch using only the Numpy package for matrix-vector operations. In order to train the network on several datasets, you will need to implement the back-propagation algorithm with gradient descent. Before getting started, let's review the notations we will use in this lab and recall the stochastic gradient descent algorithm.\n",
    "\n",
    "## Notation\n",
    "\n",
    "We will consider simple feed-forward networks that can be described by the following recursive relationship,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "&z^l = a^{l-1} W^l + b^l,\\\\\n",
    "&a^l = \\sigma^l(z^l),\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $a^l$ is the output (activation) of layer $l$ which is a nonlinear function $\\sigma^l$ of a linear transformation of the previous layer's output.  The linear transformation is performed using the weight matrix $W^l$ and bias vector $b^l$ associated with the layer $l$.  We will denote the last layer in the network with a capital $L$ superscript.  The recursion is stopped by setting $a^0 = x$, where $x$ is the input vector to our network.  Note that in the recursive expressions above, we implicitly assume that our input/output vectors are row vectors.  The reason for this will be apparent later.\n",
    "\n",
    "Taking this notation into account, we see that a network with $L-1$ hidden layers is fully expressed by its $L$ weight matrices, bias vectors, and activation functions.  We can denote the set of trainable parameters in our network by $\\theta = \\{ W^1, \\dots, W^L, b^1, \\dots, b^L \\}$.\n",
    "\n",
    "**Beware that in Python, weights', biases' and activations' indices will start at 0**.\n",
    "\n",
    "In this lab, we are only concerned with supervised learning tasks.  Recall that in supervised learning, we have a dataset represented by a list of $(x, y)$ pairs where $x$ is the input to our model and $y$ is the desired output.\n",
    "\n",
    "- For regression problems where we want to fit a function $y = f(x)$, $x$ is the independent variable vector, and $y$ is the function value.\n",
    "- In classification problems, $x$ will correspond to a set of attributes and $y$ the corresponding label.\n",
    "\n",
    "The goal of supervised learning is to \"train\" our network by adjusting its parameters in order to minimize a cost function over the entire training set,\n",
    "\n",
    "$$\n",
    "\\min_\\theta \\mathcal{L} = \\min_\\theta \\sum_{i=1}^N \\ell^{(i)}\n",
    "$$\n",
    "\n",
    "where $\\ell^{(i)} = \\ell(\\sigma^L(\\sigma^{L-1}(\\dots(x^{(i)}W^1 + b^1)\\dots)W^L + b^L), y^{(i)})$ and $\\ell(\\hat{y}, y)$ denotes the particular form of the loss function being considered.  In this lab, we will use 2 different loss functions:\n",
    "\n",
    "1. Quadratic Loss: $\\ell(\\hat{y}, y) = \\|\\hat{y} - y\\|^2$\n",
    "2. Cross-entropy Loss: $\\ell(\\hat{y}, y) = -[ y \\ln\\hat{y} + (1-y)\\ln(1-\\hat{y})]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "f70A4AU__CzM",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e69dd34bc58b78e3602de198b57680cd",
     "grade": false,
     "grade_id": "cell-6ac37293d56527cf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Step 1: Build an untrainable network\n",
    "\n",
    "Understanding (and implementing) the back-propagation algorithm can seem a little daunting at first. Therefore, let's start by building out the functions we need just to create a network that cannot be trained, but can predict (feedforward pass). First load the libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "editable": false,
    "id": "PBkd3Edr_CzM",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "806f3d139001a9553ed025eebdd357e2",
     "grade": false,
     "grade_id": "cell-9925427eca70b36f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from typing import List, Type\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=RuntimeWarning)  # s.t. you don't freak out when it's just a harmless warning\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3f11b1ac8115c056a001a8f917921fc8",
     "grade": false,
     "grade_id": "cell-1350577476da9ea6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We wish to implement activation functions ($\\sigma^\\ell$ in the notations above) as classes, to be able to also incorporate their derivatives (we will need them for back-propagation).\n",
    "\n",
    "However, we don't want multiple *instances* of activation function *objects* (the activation is always the same), hence we rely on *static* methods (these are \"Class-wide methods\" and do NOT depend on a particular object's values).\n",
    "\n",
    "Finally, we want to be able to call the activation function directly by its class' name, like a function.\n",
    "\n",
    "Due to all these reasons, we rely on the base class Activation below. In the next cell, we explain why it **appears** complicated (it's not)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c2b8a2c3057200fa3bace638a47a5dd7",
     "grade": false,
     "grade_id": "cell-10a3abf8585507c2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "*Python tricks:* \n",
    "- Static methods are callable with uninstanciated class objects.\n",
    "- `__call__` methods are retrieved when using parentheses with an instanciated class object.\n",
    "- However, instanciation has precedence over `__call__`, so calling a Class like another static method of that class or a function will not work (see [here](https://stackoverflow.com/questions/26793600/decorate-call-with-staticmethod)).\n",
    "- `__init__` (typically used to customize the instanciated objects of a class) cannot return anything.\n",
    "- The trick is thus to use it in conjunction with `__new__` (which creates the object and passes it to `__init__`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a7b53698488a20ae3dec92bbe2049299",
     "grade": false,
     "grade_id": "cell-7999382a4c9bf651",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class Activation:\n",
    "    \"\"\"\n",
    "    Base activation node class\n",
    "    \"\"\"\n",
    "\n",
    "    def __new__(self, x: float) -> float:\n",
    "        \"\"\"\n",
    "        What to do when instantiating an object: this basically says Activation(x)\n",
    "        will be equivalent to Activation()(x).\n",
    "\n",
    "        :param float or np.array x: input of the activation\n",
    "        :return: call the activation function\n",
    "        :rtype: float or np.array\n",
    "        \"\"\"\n",
    "        return self.__call__(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "YQCWlxli_CzQ",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e207dcb32c0deac960d7e0967c7aa928",
     "grade": false,
     "grade_id": "cell-7ecb4be9dc4bc9aa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exercise 1: Implement the activation functions\n",
    "\n",
    "1. The easiest activation we can implement is the identity function which simply returns the input as itself.  Implement this below in the class template `Identity`. The `prime` function should implement the derivative of the activation.\n",
    "\n",
    "*Python tricks*: `Identity` inherits from `Activation`; thus, calling `Identity(some_array)` will call `Identity.__new__(self, some_array)` which in turn will call the static method `Identity.__call__(some_array)`.\n",
    "\n",
    "*Python hint:* see [`np.ones_like`](https://numpy.org/doc/stable/reference/generated/numpy.ones_like.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "19de44288fa4c3acd5e52fa73df42465",
     "grade": false,
     "grade_id": "cell-97b01d88950ba157",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class Identity(Activation):\n",
    "    \"\"\"\n",
    "    Identity activation node\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def __call__(x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Implements the identity activation function\n",
    "\n",
    "        :param np.array x: input of the activation\n",
    "        :return: identity of input\n",
    "        :rtype: np.ndarray\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    @staticmethod\n",
    "    def prime(x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Implements the derivative of the identity activation function\n",
    "\n",
    "        :param np.array x: input of the activation\n",
    "        :return: derivative of input\n",
    "        :rtype: np.ndarray\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "de419a6f0bde69061daccfb05c4d78aa",
     "grade": true,
     "grade_id": "cell-60b6df8c945c757f",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert Identity(np.array([1])) == np.array([1])\n",
    "print(Identity(np.array([1, 1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "035e969b61f88e94f30f12845f6c62ec",
     "grade": false,
     "grade_id": "cell-a1eb8ef5a0c2aa25",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "2. The threshold activation function takes an input and returns 1 if the input is positive, otherwise 0. Implement the `Threshold` class below.\n",
    "\n",
    "*Python hint:* see [`np.where`](https://numpy.org/doc/stable/reference/generated/numpy.where.html), and [`np.zeros_like`](https://numpy.org/doc/stable/reference/generated/numpy.zeros_like.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2d3c8f83648f77f326032495b83d13f0",
     "grade": false,
     "grade_id": "cell-03e8948b196c4ddc",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class Threshold(Activation):\n",
    "    \"\"\"\n",
    "    Threshold activation node\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def __call__(x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Implements the threshold activation function\n",
    "\n",
    "        :param np.ndarray x: input of the activation\n",
    "        :return: if x > 0, 1, else 0\n",
    "        :rtype: np.ndarray\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    @staticmethod\n",
    "    def prime(x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Implements the derivative of the threshold activation function\n",
    "\n",
    "        :param np.ndarray x: input of the activation\n",
    "        :return: derivative of input\n",
    "        :rtype: np.ndarray\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "958793c7d923f70791c657ef7250fb5b",
     "grade": true,
     "grade_id": "cell-dbf41f4e682018a6",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert Threshold(np.array([0.5])) == np.array([1])\n",
    "print(Threshold(np.array([-0.1, 0.1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0cfd5502b34f433488df2f96ed0e3562",
     "grade": false,
     "grade_id": "cell-f9a0356d080c3ad0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "3. Recall that the sigmoid function is given by $\\sigma(x) = \\dfrac{1}{1 + \\exp(-x)}$. Calculate its derivative.\n",
    "\n",
    "*Recall* that the derivative of $\\dfrac{1}{f(x)}$ is $-\\dfrac{f'(x)}{f(x)^2}$. Rearrange the resulting expression so that only $\\sigma$ appears.\n",
    "\n",
    "**Expected qualitative answer in the cell below marked \"YOUR ANSWER HERE\"**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1ca1c7c3ec338e706eae6c3b93dd364b",
     "grade": true,
     "grade_id": "cell-fd3334fa63acd5fd",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4295094b5635df5c64346df838963baa",
     "grade": false,
     "grade_id": "cell-b136c1c98c1b8aad",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "4. Implement the `Sigmoid` class below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "affc3e478d9ad486f800b7ca7e3691f0",
     "grade": false,
     "grade_id": "cell-b829708eeb7af423",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class Sigmoid(Activation):    \n",
    "    \"\"\"\n",
    "    Threshold activation node\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def __call__(x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Implements the sigmoid activation function\n",
    "\n",
    "        :param np.ndarray x: input of the activation\n",
    "        :return: sigmoid transform of input\n",
    "        :rtype: np.ndarray\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @staticmethod\n",
    "    def prime(x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Implements the derivative of the sigmoid activation function\n",
    "        (you can make good use of Sigmoid(...)!)\n",
    "\n",
    "        :param np.ndarray x: input of the activation\n",
    "        :return: derivative of input\n",
    "        :rtype: np.ndarray\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e7a0899cc5a8d8c170f4d35aefa9d035",
     "grade": true,
     "grade_id": "cell-d141c4feaa9ed5dc",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert Sigmoid(np.array([0])) == np.array([0.5])\n",
    "print(Sigmoid(np.array([-1, 0, 1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "xoN3Pbi__CzU",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1df57277a5eb4b3ce33aae5ca11d3b2b",
     "grade": false,
     "grade_id": "cell-1e3ef5e338abc605",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "4. Plot the activation functions and their derivative on the domain [-5, 5] with the function below. Are the graphical results satisfactory? **(no answer expected)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "editable": false,
    "id": "cNmcyS4Q_CzU",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "819c5e1912691f68c6e85312758d9679",
     "grade": false,
     "grade_id": "cell-868d89397737e596",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def plot_activation(activation: Type[Activation]):\n",
    "    x = np.arange(-5, 5, 0.01)\n",
    "    plt.plot(x, activation(x))\n",
    "    plt.plot(x, activation.prime(x))\n",
    "    plt.legend((activation.__name__, activation.__name__ + \" prime\"))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c8f59408a1175f66919487ac6272deea",
     "grade": false,
     "grade_id": "cell-033f9a0310c31056",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "plot_activation(Identity)\n",
    "plot_activation(Threshold)\n",
    "plot_activation(Sigmoid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "8alCasgu_CzZ",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7f22ca022bd23f12af5ba9d7d3a75583",
     "grade": false,
     "grade_id": "cell-2b126872894e4635",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exercise 2: the Network class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "uhCd0Nmr_CzW",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cf9ab616a1068d12ebb9e72481571cec",
     "grade": false,
     "grade_id": "cell-9cabe34a6f94db22",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "During this lab, we will build on the following python class called `Network` which will represent our neural net.  The `Network` class will keep track of the weights, biases, and activations needed to evaluate and train our model.  The following exercises will guide you through building up the class from the skeleton below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "id": "eY-JPx-3_CzX",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "512f22c09f58fd0ece96b43cb50b724a",
     "grade": false,
     "grade_id": "cell-70244115571606c2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class Network:\n",
    "    \"\"\"\n",
    "    A simple implementation of a feed-forward artificial neural network.\n",
    "    Work on this class throughout the entire exercise, rerunning this cell after each update.\n",
    "    \"\"\"\n",
    "    def __init__(self, sizes: List[int], sigmas: List[Type[Activation]]):\n",
    "        \"\"\"\n",
    "        Construct a network given a list of the number of neurons in each layer and \n",
    "        a list of activations which will be applied to each layer.\n",
    "        \n",
    "        :param list sizes: A list of integers representing the number of nodes in each layer, \n",
    "            including the input and output layers.\n",
    "        :param list activations: A list of callable objects representing the activation functions.\n",
    "            Its size should be one less than sizes.\n",
    "        \"\"\"\n",
    "        # Exercise 2.2\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def num_params(self) -> int:\n",
    "        \"\"\"\n",
    "        Returns the total number of trainable parameters in this network.\n",
    "        This is in a sense the total number of individual entries that one can change to tweak the output.\n",
    "        \"\"\"\n",
    "        # Exercise 2.3\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def feed_forward(self, x: np.ndarray) -> (np.ndarray, list, list):\n",
    "        \"\"\"\n",
    "        Evaluates the network for the given input and returns the result.\n",
    "        \n",
    "        :param numpy.ndarray x: A numpy 2D-array where the columns represent input variables\n",
    "            and rows represent independent samples.\n",
    "        :return: output of the network, list of activations, list of zs\n",
    "        :rtype: numpy.ndarray, list, list\n",
    "        \"\"\"\n",
    "        # Exercise 2.4\n",
    "        a = [x]  # we start with a^0 = x, the input\n",
    "        zs = []\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        return a[-1], a, zs\n",
    "\n",
    "    def back_propagation(self, x: np.ndarray, y: np.ndarray, cost, verbose: bool = False) -> (list, list):\n",
    "        \"\"\"\n",
    "        Compute gradients of cost function w.r.t network parameters for a single training point.\n",
    "\n",
    "        :param numpy.ndarray x: a single input in a 2D array (1, #features), or possibly\n",
    "            several inputs in a 2D array (#samples, #features)\n",
    "        :param numpy.ndarray y: a single output\n",
    "        :param Cost cost: a cost class\n",
    "        :param bool verbose: whether to print debugging information (you don't have to implement this feature)\n",
    "        :return: list of gradients of weights, list of gradients of biases\n",
    "        :rtype: list, list\n",
    "        \"\"\"\n",
    "        # Exercise 1.2\n",
    "        L = len(self.sizes)\n",
    "        _, list_a, list_z = self.feed_forward(x)\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        return w_grad, b_grad\n",
    "\n",
    "    def update_step(self, x_train: np.ndarray, y_train: np.ndarray, cost, learning_rate: float,\n",
    "                    verbose: bool = False):\n",
    "        \"\"\"\n",
    "        Compute the average parameter gradients over the whole training set and do a GD step.\n",
    "        N.B.: this does not return anything!\n",
    "\n",
    "        :param numpy.ndarray x_train: A design matrix (2D array (#samples, #features))\n",
    "        :param numpy.ndarray y_train: A vector / array of responses\n",
    "        :param Cost cost: A callable object with signature cost(y_pred, y) for computing the cost of a single\n",
    "        training example.\n",
    "        :param float learning_rate: The learning rate to use in the GD step.\n",
    "        :param bool update_step: Perform an update step: average all the gradients and update the parameters.\n",
    "        \"\"\"\n",
    "        # Exercise 1.3\n",
    "        ## Suggestions\n",
    "        # Compute gradients of weights and biases for all layers and samples\n",
    "        # Update the weights and biases\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def train(self, x_train, y_train, cost, learning_rate: float, epochs: int = 100, verbose: bool = True):\n",
    "        \"\"\"\n",
    "        Trains this network on the training data, i.e. iterate the update step.\n",
    "        N.B.: this does not return anything!\n",
    "\n",
    "        :param np.ndarray x_train: A design matrix\n",
    "        :param np.ndarray y_train: A vector of responses\n",
    "        :param Cost cost: A callable object with signature cost(y_pred, y) for computing the cost of a single\n",
    "        training example.\n",
    "        :param float learning_rate: The learning rate to use in the GD step.\n",
    "        :param int epochs: The number of epochs to train with.\n",
    "        :param bool verbose: Whether to print the loss at each epoch.\n",
    "        \"\"\"\n",
    "        for i in range(epochs):\n",
    "            self.update_step(x_train, y_train, cost, learning_rate)\n",
    "\n",
    "            # Compute the loss\n",
    "            y_pred, _, _ = self.feed_forward(x_train)\n",
    "            loss = cost(y_pred, y_train)\n",
    "            self.loss.append(loss)\n",
    "            if verbose:\n",
    "                print('epoch:', i, '\\n\\tloss: ', loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8307f27f3939493267aab4a8566aa813",
     "grade": false,
     "grade_id": "cell-3346888b14e4bb36",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "1. From the recursion formulas above, write down the shapes of the weight matrices and bias vectors, given the number of neurons $d^l$ in layer $l$. **Expected concise qualitative answer in the cell below marked \"YOUR ANSWER HERE\"**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "15df1237c9d043e47cdeca9ec273ea97",
     "grade": true,
     "grade_id": "cell-0821a88865b736d1",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f0a8f7424024a6ec11f69aaaf4cbf902",
     "grade": false,
     "grade_id": "cell-95e2712e5b603757",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "2. Implement the init `__init__` function in `Network` **above**, which is tested **below**.\n",
    "\n",
    "* Store the provided `sizes` (a list of size $L + 1$ representing the respective size $d^l$ of each layer including the input which will be given as a parameter) and `sigmas` (a list of size $L$ of callable functions representing the activation function in each layer which will be given as a parameter) in attributes `sizes` and `sigmas`;\n",
    "\n",
    "* Construct a list of weight matrices (in a `weights` attribute), a list of bias vectors (in a `biases` attribute) which are randomly initialized from the standard normal distribution (see `np.random.randn`); pay attention to the size of each list and the shape of each element which must be consistent with `sizes`.\n",
    "\n",
    "*Python hints*: `array[1:]` gives you all the array except the first entry; `array[:-1]` gives you all the array except the last entry; `for x, y in zip(a, b)` lets you iterate \"simultaneously\" through a pair of objects of same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dc05bd26ec2f132438104b043b262093",
     "grade": true,
     "grade_id": "cell-90370fca040d6c63",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test the network's initialization\n",
    "assert Network([1], Sigmoid).sizes == [1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1db25b477aa54df86f61c6ce1e58700c",
     "grade": false,
     "grade_id": "cell-1abf1a64a6747dd4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "3. Implement the `num_params` function in `Network` **above** which is tested **below**. You may use your knowledge of the shapes of the weights and biases given the `sizes` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3850aee0ed3652348b4027d689ee4a4d",
     "grade": true,
     "grade_id": "cell-b870d6b966b394b7",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test the network's number of parameters\n",
    "assert Network([1, 2, 2, 1], [Sigmoid, Sigmoid, Sigmoid]).num_params() == 13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5e1b181b8ed25fa818add7a127a6e36c",
     "grade": false,
     "grade_id": "cell-ebb67909ec605029",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "4. Implement the `feed_forward` function in `Network` **above** which is tested **below**. Use the recurrence relations discussed in the Notation section, in particular:\n",
    "\n",
    "$$z^l = a^{l-1} W^l + b^l,\\\\\n",
    "a^l = \\sigma^l(z^l).$$\n",
    "\n",
    "Return:\n",
    "* the output of the network, *i.e.* $a^L$.\n",
    "* the list of $a^l$s (*i.e.* $[a^0, \\dots, a^L]$).\n",
    "* the list of linear combinations $z^l$s (*i.e.* $[z^1, \\dots, z^L]$).\n",
    "\n",
    "The two last lists returned will serve for the back-propagation implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "48de766afa38ceae18a7e0935ec1cf68",
     "grade": true,
     "grade_id": "cell-f18884d1944a1976",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test the network's feedforward result\n",
    "output, _, _ = Network([1, 2, 2, 1], [Sigmoid, Sigmoid, Sigmoid]).feed_forward(np.array([[1]]))\n",
    "assert output.shape[0] == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "gSel8dO__Cza",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5e9167eb94427a79de589c98abd14f64",
     "grade": false,
     "grade_id": "cell-4bddc8c9c4cbed49",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "5. Create a `network` with 4 inputs, 2 hidden layers of 5 nodes each, and 1 output (use any activation). This network could typically be used with the `iris` dataset, since we have 4 features (petal/sepal length/width) and 1 output (setosa/versicolor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "id": "11dZmuk5_Czb",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "63c192afcd5aba6cd9490ef93c106420",
     "grade": false,
     "grade_id": "cell-f4d5798cb8807a54",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# network = ...  # <- TO UNCOMMENT AND COMPLETE\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "O1HGWxSg_Czh",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cbdb5708cfa04a8c8a61267052252be0",
     "grade": false,
     "grade_id": "cell-99327dd1116cd912",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "6. Print the weights and biases of the network and confirm they are intialized correctly (shape and values). **(no qualitative answer expected)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "editable": false,
    "id": "ri_SV2x0_Czi",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f0e1de9ecc086523fdf14ea65a0f5797",
     "grade": true,
     "grade_id": "cell-98afd0b9734c77a8",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "for index, weight in enumerate(network.weights):\n",
    "    print(f\"\\nW^{index}\")\n",
    "    print(\"Shape:\", weight.shape)\n",
    "    print(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "40d2cbcabe422f44780b364551037aa3",
     "grade": true,
     "grade_id": "cell-034bea04084ae05d",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "for index, bias in enumerate(network.biases):\n",
    "    print(\"\\nBias\", index)\n",
    "    print(\"Shape:\", bias.shape)\n",
    "    print(bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "VXTC5Gpe_Czm",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "341d2e6ed9b0f813c20ad3c25c38ba0c",
     "grade": false,
     "grade_id": "cell-9285ed448929d454",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exercise 3: Build logic gates\n",
    "\n",
    "Below is a table of logical functions (logic gates).  Each function takes two values (A and B) representing True (1) or False (0) propositions and returns a True or False value.\n",
    "\n",
    "| A | B | AND | OR | XOR | NAND |\n",
    "|---|---|-----|----|-----|------|\n",
    "| 0 | 0 | 0   | 0  | 0   | 1    |\n",
    "| 0 | 1 | 0   | 1  | 1   | 1    |\n",
    "| 1 | 0 | 0   | 1  | 1   | 1    |\n",
    "| 1 | 1 | 1   | 1  | 0   | 0    |\n",
    "\n",
    "Interestingly, [it is possible to create any boolean function of any size through a combination of NAND gates](https://en.wikipedia.org/wiki/NAND_gate)!  Thus, if we can create a network which reproduces the logic behind a NAND-gate, it is possible to represent any logical function (and by extension any mathematical function) by combining such a network into ever more complex networks.  This is one version of a universal approximation theorem for ANNs.\n",
    "\n",
    "**Task:** build the AND, OR, and NAND logic gates above using the simple network below by **modifying its weights and biases directly**. The `logic_gate` function is provided to test your networks: it corresponds to columns A, B in the table above. We will test if we get the expected results (columns AND, ..., NAND)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4f5feced4ce1b1cae18bfba0a10ff7c4",
     "grade": false,
     "grade_id": "cell-3718598f00e4622d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def logic_gate(network: Network, array: np.array = np.asarray([\n",
    "            [0, 0],\n",
    "            [0, 1],\n",
    "            [1, 0],\n",
    "            [1, 1]])) -> np.array:\n",
    "    \"\"\"\n",
    "    Helper function to test our network as a logic gate.\n",
    "\n",
    "    :param Network network: our neural network\n",
    "    :param numpy.array array: apply feed forward to array\n",
    "    :return: output of network given array\n",
    "    :rtype: numpy.array\n",
    "    \"\"\"\n",
    "    output, _, _ = network.feed_forward(array)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "79f5ef1ddb2873f7fe2ee2b974a546ce",
     "grade": false,
     "grade_id": "cell-155227b9348f53c9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Create a simple \"logic gate\" network with two inputs A and B and 1 output, with activation function Threshold\n",
    "network = Network([2, 1], [Threshold])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "06a272ffb2d4ae6825d91171fbce12a6",
     "grade": false,
     "grade_id": "cell-b6d135b9536dd76e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "1. Implement the AND gate by modifying the `network`'s weights and biases.\n",
    "\n",
    "*Hint:* what should $W$ and $b$ be such that we get `[0, 0, 0, 1]` when the input is (A, B)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "id": "Qp7utpCB_Czn",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e37ce797e6676a9b2386a3b1a19204e5",
     "grade": false,
     "grade_id": "cell-a42131ad5e265fef",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# AND gate\n",
    "# network.weights[0] =  ...\n",
    "# network.biases[0] = ...\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4ee905c885fd0881cff13de556e0ce11",
     "grade": true,
     "grade_id": "cell-846b9303cb272ebd",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test if we get the expected result (see table above)\n",
    "np.testing.assert_array_equal(logic_gate(network), np.array([0, 0, 0, 1]).reshape((-1, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5f49f86c3d452925debbd60472229c8a",
     "grade": false,
     "grade_id": "cell-4725d6fb651243c0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "2. Implement the OR gate by modifying the `network`'s weights and biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "61180b45dd694b32598d974a6c64e73a",
     "grade": false,
     "grade_id": "cell-480fe471b4467d26",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# OR gate\n",
    "# network.weights[0] = ...\n",
    "# network.biases[0] = ...\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fa65a7636e93a5543e94402b3b87ac04",
     "grade": true,
     "grade_id": "cell-846b9303cb272ebc",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test if we get the expected result (see table above)\n",
    "np.testing.assert_array_equal(logic_gate(network), np.array([0, 1, 1, 1]).reshape((-1, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "87bda956c6842571606f991c2be92f7c",
     "grade": false,
     "grade_id": "cell-06c4fc60e02fda82",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "3. Implement the NAND gate by modifying the `network`'s weights and biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "581f3b4a3ce0f07f4f510f8ec82f8405",
     "grade": false,
     "grade_id": "cell-bfa280b436bf2d7a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# NAND gate\n",
    "# network.weights[0] = ...\n",
    "# network.biases[0] = ...\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "57546bf42555b02ac2810b6a4ba2bd89",
     "grade": true,
     "grade_id": "cell-846b9303cb272ebe",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test if we get the expected result (see table above)\n",
    "np.testing.assert_array_equal(logic_gate(network), np.array([1, 1, 1, 0]).reshape((-1, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "yiF9J9OA_Czr",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "52f1e82d70ea40edfda60cb1de45df5a",
     "grade": false,
     "grade_id": "cell-61601a225ff5885a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Step 2: Training the network\n",
    "\n",
    "While interesting, the `Network` class above is pretty useless as it stands since there is no way to learn a function we don't already know.  In this step, we will add the ability to train our network on a dataset using the gradient descent and back-propagation algorithms.  Let's review both of these algorithms now.\n",
    "\n",
    "### (Stochastic) Gradient Descent\n",
    "\n",
    "Recall from the beginning of the lab that we want to train the network parameters by minimizing a given loss function over an entire dataset. One method of doing this is using the gradient descent (GD) algorithm which you have already seen in the lectures and previous labs; recall the update rule:\n",
    "\n",
    "$$\n",
    "\\theta = \\theta - \\eta \\nabla_\\theta \\mathcal{L},\n",
    "$$\n",
    "\n",
    "where $\\eta$ is the learning rate.  The update rule above is called GD because direction of change in the network parameters follow the opposite of the parameters' gradient in the loss function.  You can think of this like a ball rolling down a hill to find the minimum of the topology. Only in this case, the ball is massless because it has no momentum. Note that, contrary to linear and logistic regression, the loss / error function might not be convex, hence the existence of a unique minimizer isn't guaranteed, nor is it guaranteed that gradient descent will find it.\n",
    "\n",
    "When the update rule is applied to a random subset of the total dataset, it is called stochastic gradient descent (SGD). SGD is far more efficient (computationally) than GD when the batch size is large enough to approximate the true gradients while being significantly smaller than the full dataset. Running over the entire dataset with SGD once is called an \"epoch\" (of training).\n",
    "\n",
    "### Back-Propagation\n",
    "\n",
    "From the GD update rule above, it is clear we will need to compute the gradients of the network parameters with respect to the cost function. This is exactly what back-propagation does, and thus is a crucial component to almost all neural network learning algorithms. In the next exercise, we will derive the 4 equations in back-propagation. You will need knowledge of the [chain rule for differentiation](https://en.wikipedia.org/wiki/Chain_rule) if you are not already familiar with this.\n",
    "\n",
    "### Exercise 4: Derive backprop formulas\n",
    "\n",
    "Before we start, it is convenient to define the following variable:\n",
    "\n",
    "$$\n",
    "\\delta^l \\equiv \\frac{\\partial \\ell_p}{\\partial z^l}.\n",
    "$$\n",
    "\n",
    "In other words, $\\delta^l$ is the gradient of the loss function for a point $p$ with respect to the input to the activation function for the layer $l$ in our network.\n",
    "\n",
    "**Answer the following qualitative questions, respectively in each cell that comes just below the question, where it says *YOUR ANSWER HERE***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0499d5a0056e7853f8c36be38a0652ce",
     "grade": false,
     "grade_id": "cell-9a2cffdf102ab517",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "1. What is the shape of $\\delta^L$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d3c2180f3932dc699f445df1f7481df2",
     "grade": true,
     "grade_id": "cell-865769c85dc2a9d9",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "69d72d5b65158dae365276cb97af1c8f",
     "grade": false,
     "grade_id": "cell-0858c5f882d20f8f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "2. Show that $\\delta^L = \\nabla_{a^L} \\ell_p  \\odot {\\sigma'}^L ( z^L )$.\n",
    "\n",
    "*Hint:* apply the chain rule to the definition of $\\delta^L$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "21fee3ea20b5bdc41787027a873436d7",
     "grade": true,
     "grade_id": "cell-c6710eca7f38cfd4",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cd6a163a7815888a7f7f8dcd15a2652a",
     "grade": false,
     "grade_id": "cell-1535ada7abc45b49",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "3. Show that for $l < L$, $\\delta^l = [\\delta^{l+1} (W^{l+1})^T ] \\odot {\\sigma'}^l ( z^l )$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "412cd8077510fb9c417f756175727513",
     "grade": true,
     "grade_id": "cell-727fb651f4009ba5",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d3fd692815dc5549ed95912515871c4c",
     "grade": false,
     "grade_id": "cell-8feb9a660f56fe91",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "4. Show that $\\nabla_{W^l}\\ell_p = (a^{l-1})^T \\delta^l$.\n",
    "\n",
    "*Hint:* use the definition of $z^l$ and derive w.r.t. a single component $W_{i,j}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8e1832939317e82bfdc448ebaa1bb86f",
     "grade": true,
     "grade_id": "cell-4f2e22170c5f182e",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f73f96c72665a5c9ddfd937e3ce6ad19",
     "grade": false,
     "grade_id": "cell-3e26d4c6bbded646",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "5. Show that $\\nabla_{b^l}\\ell_p = \\delta^l$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a579082f8e87d1b5bc1ad1bb37b4e4a2",
     "grade": true,
     "grade_id": "cell-1612d709394748c4",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "145a03bc2db3c699b2ee298aa177349d",
     "grade": false,
     "grade_id": "cell-77d5ea6f4ae46501",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Note that these four last formulas allow us to compute the gradient of the loss function for a single training point with respect to the parameters of our network.  Looking at the equations more closely, you should see a possible algorithm form.\n",
    "\n",
    "1. Compute the values of $z^l$ and $a^l$ for $l = {1, \\dots, L}$ by forward propagation through the network.  Recall $a^0 = x_p$: that's what we've implemented throughout Exercises 1-4.\n",
    "2. Use the equation in Exercise 5.2 above to compute $\\delta^L$.  \n",
    "3. Back-propagate in the reverse direction using the equation in Exercise 5.3 to get all the other $\\delta^l$ values.\n",
    "4. Compute the parameter gradients using the equations in Exercises 5.4 and 5.5.\n",
    "\n",
    "Note that this will yield the gradients for a single training point.  The gradients for the total loss function can easily be computed by\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta \\mathcal{L} = \\sum_{p=1}^N \\nabla_\\theta \\ell_p.\n",
    "$$\n",
    "\n",
    "Of course, the exact form of $\\nabla_\\theta \\ell_p$ will depend on the choice of $\\ell$, so let's discuss that now!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "CcgyCRvg_Czs",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4a3cae9e92c70ba430c251443c603fc3",
     "grade": false,
     "grade_id": "cell-d27916f41d987521",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Exercise 5: Implement the loss functions\n",
    "\n",
    "Before we can implement the training algorithm, we need to define a cost function. For now, let's create a base class `Cost`, similar to the base class `Activation`, for simplicity, and a `QuadraticCost` class representing the quadratic cost **which was given in the introduction**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c175f337197dfff908230661a8c60bf7",
     "grade": false,
     "grade_id": "cell-31f2fc00dfe98dd4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class Cost:\n",
    "    def __new__(self, y_pred: np.array, y: np.array) -> float:\n",
    "        return self.__call__(y_pred, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a376e2230a24590953ed1a1e2fe72dfb",
     "grade": false,
     "grade_id": "cell-a25929072153e7fc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "1. Implement the quadratic cost in the function below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "77ba5ec145a1d916e22ed006a0225905",
     "grade": false,
     "grade_id": "cell-5a04becd2b8926a2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class QuadraticCost(Cost):\n",
    "    @staticmethod\n",
    "    def __call__(y_pred: np.array, y: np.array) -> float:\n",
    "        \"\"\"\n",
    "        Computes the quadratic cost function.\n",
    "\n",
    "        :param numpy.array y_pred: y_hat for each sample\n",
    "        :param numpy.array y: y for each sample\n",
    "        :return: Squared L2 norm of the difference\n",
    "        :rtype: float\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    @staticmethod\n",
    "    def prime(y_pred: np.array, y: np.array) -> np.array:\n",
    "        \"\"\"\n",
    "        Computes the derivative of the quadratic cost function.\n",
    "\n",
    "        :param numpy.array y_pred: y_hat for each sample\n",
    "        :param numpy.array y: y for each sample\n",
    "        :return: derivative of cost function w.r.t. y_pred\n",
    "        :rtype: numpy.array\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "307ccae8d9f7d40374377d0f49ad220d",
     "grade": true,
     "grade_id": "cell-ca3d45c72702b75c",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert QuadraticCost(np.array([0]), np.array([0])) == 0\n",
    "assert QuadraticCost.prime(np.array([0]), np.array([0])) == 0\n",
    "assert QuadraticCost(np.array([0, 0]), np.array([0, 0])) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a2f62ba8fdba626e20dedd3c5da8cb08",
     "grade": false,
     "grade_id": "cell-da7a9244a1bf379a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "2. Implement the cross-entropy loss function below **which was given in the introduction**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1693ae723dc0216bfd0065c5c051e531",
     "grade": false,
     "grade_id": "cell-480238ada0e29535",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class CrossEntropy(Cost):\n",
    "    \"\"\"Cross Entropy loss function\"\"\"\n",
    "    @staticmethod\n",
    "    def __call__(y_pred: np.array, y: np.array) -> float:\n",
    "        \"\"\"\n",
    "        Value of the cross entropy loss given predictions and true labels\n",
    "\n",
    "        :param numpy.array y_pred: 1D array associated with the predicted probability of y=1\n",
    "        :param numpy.array y: 1D array of true (0/1) classes\n",
    "        :return: value of the loss\n",
    "        :rtype: float or numpy.array\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @staticmethod\n",
    "    def prime(y_pred: np.array, y: np.array) -> np.array:\n",
    "        \"\"\"\n",
    "        Derivative of the cross entropy loss function\n",
    "\n",
    "        :param numpy.array y_pred: 1D array associated with the predicted probability of y=1\n",
    "        :param numpy.array y: 1D array of true (0/1) classes\n",
    "        :return: gradient of the loss w.r.t. y_pred\n",
    "        :rtype: numpy.array\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fd3197ff193dfabcee91ee28401d72a6",
     "grade": true,
     "grade_id": "cell-edf98faca095c127",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert CrossEntropy(np.array([0.5]), np.array([0])) == - np.log(0.5)\n",
    "assert CrossEntropy.prime(np.array([0.5]), np.array([0])) == 2.0\n",
    "assert int(CrossEntropy(np.array([0.5, 0.5]),\n",
    "                    np.array([0, 1]))) == 1\n",
    "assert CrossEntropy.prime(np.array([0.5]), np.array([0])) == 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6e41943ec82f6c771bb1d41d27b6f325",
     "grade": false,
     "grade_id": "cell-ec14d64fcdb5c5ad",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Exercise 6: implement the training logic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6716cc87a65f97026a91159091579451",
     "grade": false,
     "grade_id": "cell-6fb47591ecddaa6e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "1. Back-propagation\n",
    "\n",
    "With this cost defined, let's implement three remaining functions in `Network` that will allow us to train our models. The first is the `back_propagation` method which takes either a single training sample $x, y$ and a `Cost` class like the one above, and computes the gradient of the cost function w.r.t. to all the weights and biases using the backpropagation algorithm, or a dataset $(x^{(i)}, y^{(i)})$ and computes the **sum** of the gradient of the cost w.r.t. to all the weights and biases (which will simplify Exercise 1.3 below). Use the formulas found at the \"Back-Propagation\" Section (which are also in your lecture notes...). It sould return the lists of $\\nabla_{W^l}\\ell_p$ and $\\nabla_{b^l}\\ell_p$ for $1 \\leq l \\leq L$.\n",
    "\n",
    "**Implement this in the `Network` class above, remember to execute the cell where the `Network` class is defined before running the tests below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "000269abd652dd6294fbb73b5169eb58",
     "grade": false,
     "grade_id": "cell-f1e3870206c6c0c3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Define a single training point\n",
    "x = np.array([1, -1]).reshape(1, 2)\n",
    "y = np.array([0])\n",
    "\n",
    "# Define a network\n",
    "network = Network([2, 5, 1], [Sigmoid, Identity])\n",
    "\n",
    "# Perform back-propagation\n",
    "w_grad, b_grad = network.back_propagation(x, y, QuadraticCost)\n",
    "\n",
    "assert len(w_grad) == 2  # 2 layers (1 hidden, 1 output)\n",
    "assert len(b_grad) == 2  # 2 layers (1 hidden, 1 output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5f6d9fabf8cc4a111d6f94436a6bce69",
     "grade": true,
     "grade_id": "cell-64ae458ce7c19a1b",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bfb798b0ab07af4538ccdb77f89ad74a",
     "grade": false,
     "grade_id": "cell-856399c35775b2b7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "2. Implement the `update_step` function in the `Network` class.\n",
    "\n",
    "Next, implement a single epoch of training by **summing** the gradients over the whole dataset and using the GD update rule to update the weights and biases.\n",
    "\n",
    "The `update_step` method is used by the `train` method successively in a loop as you can see in the `Network` class implementation above.  Assuming you have implemented the `backprop` and `update_step` functions correctly, you should now be able to train the network on real training data.\n",
    "\n",
    "**Implement this in the `Network` class above, remember to execute the cell where the `Network` class is defined before running the tests below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a0d7d290ef4bc1ec9f35c3a07d02ff83",
     "grade": true,
     "grade_id": "cell-3c1323800fdae1f6",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Generate training points\n",
    "np.random.seed(0)\n",
    "x = np.random.uniform(-1, 1, (100, 1))\n",
    "y = np.cos(np.pi * x)**2\n",
    "\n",
    "# Define a network\n",
    "network = Network([1, 100, 1], [Sigmoid, Identity])\n",
    "\n",
    "# Store weights and biases before the update to see if something changed\n",
    "weights_before = network.weights[0].copy()\n",
    "biases_before = network.biases[0].copy()\n",
    "\n",
    "\n",
    "# Perform an update step\n",
    "network.update_step(x, y, QuadraticCost, 0.1)\n",
    "\n",
    "# Assert that something did change\n",
    "assert np.any(np.not_equal(network.weights[0], weights_before))\n",
    "assert np.any(np.not_equal(network.biases[0], biases_before))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c8452d541645b21ed7c113ae0c771377",
     "grade": false,
     "grade_id": "cell-8fbab77dabd1581b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Exercise 7: Test training on a simple regression problem\n",
    "\n",
    "Run the following code to test the network.  If all is correct, you should see the total loss dropping and the network will learn the sine function.  \n",
    "\n",
    "1. Play with different hyperparameters of the network and training such as the number of layers, number of nodes per layer, learning rate, and number of training samples. Can you improve the training performance? **Qualitative answer expected in the last cell that says \"YOUR ANSWER HERE\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters to tune\n",
    "n_iterations = 1000\n",
    "lr = 0.005\n",
    "verbose = False\n",
    "sizes = [1, 2, 2, 1]\n",
    "activations = [Sigmoid, Sigmoid, Identity]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "26736183c832301c86259dfd1c9b950e",
     "grade": false,
     "grade_id": "cell-98eadfe5ef6dbfa3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Training the network\n",
    "np.random.seed(1)\n",
    "network = Network(sizes, activations)\n",
    "\n",
    "network.train(x_train=x, y_train=y, cost=QuadraticCost,\n",
    "              learning_rate=lr, epochs=n_iterations, verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5222cdb76a6879d7a369afdb113b546b",
     "grade": false,
     "grade_id": "cell-b9126ac49967f965",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Some test samples\n",
    "x_pred = np.arange(-1, 1, 0.01).reshape(200, 1)\n",
    "y_pred, _, _ = network.feed_forward(x_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "356d49900b07fb96d7270c3148d7aece",
     "grade": false,
     "grade_id": "cell-98d10fbe9a8568a5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Plot train samples, truth, test samples and predictions\n",
    "plt.plot(x, y, 'k.', label=\"Truth\")\n",
    "plt.plot(x_pred, y_pred, 'k-', label=\"Prediction\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.plot(range(n_iterations), network.loss, label=\"Training loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.plot(range(int(n_iterations / 2), n_iterations),\n",
    "         network.loss[int(n_iterations / 2):],\n",
    "         label=f\"Training loss, last {int(n_iterations / 2)} observations\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "50aba6270db207667e110abca167ff68",
     "grade": true,
     "grade_id": "cell-3938c7c1567af768",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f4b3e98cdc72b2082e115500d7ba359b",
     "grade": false,
     "grade_id": "cell-1c298b3ad97472f2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Step 3: Fashion items classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7b3f7ba1fa9c7c45b79257a71d81dbcb",
     "grade": false,
     "grade_id": "cell-a2ee702f2bd7d4eb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let's test our model on a more sophisticated classification dataset, namely [Zalando research's fashion MNIST dataset](https://github.com/zalandoresearch/fashion-mnist). This dataset includes 70,000 images of fashion items 28x28 pixels (=784), split into a training set of 60,000 images and a test set of 10,000 images. Each image is labeled with a digit from 0 to 9."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5b9fc3ea4332480e423399e54065340b",
     "grade": false,
     "grade_id": "cell-09cd90297d14a0aa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The original [MNIST dataset](http://yann.lecun.com/exdb/mnist/) consists in handwritten digits, e.g. to automatically read the addresses on mails. The performance of neural networks on this task was so much better than previous approaches that it (re-)sparked a lot of interest in them (and brought fame to Yann LeCun). It is now considered an \"easy\" task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7f8f80a5c680250a452b1aaaec998d05",
     "grade": false,
     "grade_id": "cell-6fa0f487d0bde4ff",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Exercise 8: Get to know the dataset\n",
    "\n",
    "1. Just run the following cell to download the Fashion MNIST database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2b44b7b9ea8c8095a3c9859724be8113",
     "grade": false,
     "grade_id": "cell-509b61f8d16eae94",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fad3dc95f171a37ae03a8374ecab66f2",
     "grade": false,
     "grade_id": "cell-5a84741b13cd62d6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "127f11091dd063250dfe870391568d55",
     "grade": false,
     "grade_id": "cell-9c5f7c0536263f00",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "2. Next check that the shapes of the training and test data, and their content, are what you expect. Please answer by *e.g.* printing the shapes of the training and test data as well as the ranges of values that are contained in them and comment in the next cell. **Qualitative code expected in the cell below, qualitative textual answer in the subsequent cell marked \"YOUR ANSWER HERE\"**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "666ebe2a24960836a3a9ae6d09ed5e78",
     "grade": true,
     "grade_id": "cell-27b143146b28bd45",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "88fbdb703b62905de134612fb4b8094a",
     "grade": true,
     "grade_id": "cell-c0949777ae1e66c7",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1ab39dcdcc8bac12af227e3d42803ede",
     "grade": false,
     "grade_id": "cell-bd604bb0f4fd4702",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "3. Get a feel for the dataset by displaying some images in _grayscale_. Please print one image per class together with their class label.\n",
    "\n",
    "*Python hint*: [plt.imshow](https://matplotlib.org/3.5.1/api/_as_gen/matplotlib.pyplot.imshow.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ff0a80d7e439bc6626cf8575c72bdc1f",
     "grade": true,
     "grade_id": "cell-c0f5962936f8b07a",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fded7c0988ddb9a26940cc408664c3c5",
     "grade": false,
     "grade_id": "cell-c7c55547566a6854",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Exercise 9: Create a one-hot encoding of the labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "50dfa5d18c9e8b516da3a01141ed3e7e",
     "grade": false,
     "grade_id": "cell-bba6740460854730",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "`y_train` and `y_test` currently contain the digits 0-9, which is not suited for training as we want to perform a 10-class classification training on our network. To fix this, we need to perform a so-called one-hot encoding on the labels which convert each digit into a vector of length 10 full of zeros, except the element corresponding to the digit which should be 1.  For example, if our label is 3, then the corresponding one-hot encoding is [0, 0, 0, 1, 0, 0, 0, 0, 0, 0].\n",
    "\n",
    "1. Convert the `y_train` and `y_test` vectors into one-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "642c0928678042b41729fafcd1a8637a",
     "grade": false,
     "grade_id": "cell-2869834dee056d53",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# y_train =  ...\n",
    "# y_test =  ...\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cc4b00bc13a4677e0b39f8a008f9b357",
     "grade": true,
     "grade_id": "cell-392bf398946af777",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e31c08834b4995099ddbb9e4f34b8314",
     "grade": false,
     "grade_id": "cell-2fec64430b20ccf2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Exercise 10: Train your network to recognize fashion items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cb792fca1add615c693eddcf755214f5",
     "grade": false,
     "grade_id": "cell-c263dc9304e67a65",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "1. Instantiate a network with 1 hidden layer of 32 nodes, with sigmoid activations and a cross-entropy loss. Note that this network cannot take 2-dimensional data points as input. Therefore, it should be instantiated to expect 'flattened' images as input, i.e., images that have been converted to vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "de53d41c0bdd7d2d9a330d2605ea35e1",
     "grade": false,
     "grade_id": "cell-caaa0d143910b30c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# network = ...\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "65494148f7b339b00e76f16d47b83397",
     "grade": true,
     "grade_id": "cell-a7afcd2c486f510e",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9a51ed9467750caec3768f7aa02fbf03",
     "grade": false,
     "grade_id": "cell-61ff29d4f6fdf054",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "2. Train the network using the first 10000 images in the training set, any learning rate / number of epochs (see Exercise 4.4.). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "67fb776fbdd571093ef21edda5c17c30",
     "grade": false,
     "grade_id": "cell-99c7b597e04951b9",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Train the network for X epochs with a learning rate of Y\n",
    "# Our implementation does not deal with 3D inputs, so you need\n",
    "# to \"flatten\" the 28x28 images in a single vector / observation / sample / row.\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "24af6ae040e197b22d3d7921efcc3d1a",
     "grade": true,
     "grade_id": "cell-93ca02df2ca56eab",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "94a6352f89df7948379dc302901b02c4",
     "grade": false,
     "grade_id": "cell-371a4ddf3b6399ff",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "3. Plot the evolution of the loss and go back to training, tweaking the learning rate / number of iterations until the plot looks satisfactory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a70dfaf4ada76375744a40b3fde953d9",
     "grade": true,
     "grade_id": "cell-52650364abe3bf96",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "23cd1d2cee3151185492594bc42de8d8",
     "grade": false,
     "grade_id": "cell-24cf28fe7ec1314e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "4. Compute the accuracy of the model (and put it in `accuracy`) on the test data and compare a few predictions to their images in grayscale (see labels [here](https://github.com/zalandoresearch/fashion-mnist#labels))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f7632120f506073240a7cfb427928ad5",
     "grade": false,
     "grade_id": "cell-3cee56d5cac16c9c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Compute the accuracy\n",
    "# accuracy = ...\n",
    "\n",
    "# For example, display an example of a class probability and its true value\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "13701d9a970ea413113275ecdf1d1ca6",
     "grade": true,
     "grade_id": "cell-5caf06ae42e10b0b",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7e9adaba8d1eab8ed8141bae35eed517",
     "grade": false,
     "grade_id": "cell-ece2b9dfec4c7541",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**CONGRATULATIONS**, you reimplemented a dense multilayer perceptron neural network from scratch using only NumPy, and you can (relatively) effectively classify **images** of fashion items."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Copie de Lab6.ipynb",
   "provenance": [
    {
     "file_id": "https://github.com/jbscoggi/teaching/blob/master/Polytechnique/CSE204/Lab6.ipynb",
     "timestamp": 1576227403767
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
